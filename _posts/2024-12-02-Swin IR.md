---
layout: single        # 문서 형식
title: Swin IR Image Restoration Using Swin Transformer (2021)        # 제목
categories: Semantic Segmentation   # 카테고리
tag: [DL, Image, Transformer]
toc: true             # 글 목차
toc_sticky : true     # toc 고정
toc_label: 목차       # toc 이름 설정
author_profile: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
Swin Transformer, Shifted Window, Shallow Feature Extraction, Deep Feature Extraction, Image Reconstruction


# 1. Introduction
## 1.1. CNN based Methods 
대부분의 CNN 기반 방법들은 잔차 학습, dense connection 등과 같은 구조들로 구성되는데, 일반적으로 합성곱으로 기인하는 문제점들이 발생한다. 

- 문제점 1
이미지와 합성곱 커널간 상호작용이 이미지의 내용들과 무관, 모든 이미지 영역에서 동일하게 동작

- 문제점 2
주변 픽셀들과의 연관성은 잘 처리하지만, 멀리 떨어져 있는 픽셀과의 관계를 효과적으로 다루지는 못함.


## 1.2. Transformer based Methods
CNN 의 단점들을 보완하고자 이미지 분야에도 transformer 를 도입해, 이미지의 전체적인 특징을 학습하고자 하였다. 또한 이미지 복원에서 ViT 를 사용할 때, 입력 이미지를 고정된 크기의 여러 패치들로 나눠 독립적으로 처리한다. 하지만 이 역시 다음과 같은 문제점들이 발생한다.

- 문제점 1
패치의 경계부분의 픽셀들은 패치 바깥의 픽셀들과 상호작용이 불가능

- 문제점 2
복원된 이미지에서 패치 사이 경계선에 인공적인 흔적이 발생할 가능성 존재

따라서 본 논문에서는 CNN 기반 방법의 장점인 지역적 특성을 잘 반영하는 것과 transformer 기반의 장점인 shifted window 를 이용한 멀리 떨어져 있는 픽셀과의 관계를 잘 학습한다는 것을 융합하는 Swin Transformer 를 사용해 이미지를 복원하고자 한다.



# 2.Methods
## 2.1. Overall Architecture

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2.jpg" height="225">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure1 : The Architecture of the Proposed SwinIR for Image Restoration ]</figcaption>
</figure>

Shallow 와 Deep feature extraction 모듈은 은 항상 동일하게 구성하지만, taask 에 따라 reconstruction 모듈은 다르게 구성한다.

## 2.2. Swin Transformer Layer

Swin Transformer Layer (STL) 는 기존 transformer 층의 기본적인 multi-head self-attention 에 기반한 층이다. 전체 구조를 확인하면 STL을 쌓아 Residual Swin Transformer Block (RSTB) 를 구성하고, RSTB 를 쌓아 Depp Feature Extraction 을 수행한다.


#### - Self-Attention in Local Window
본 논문에서 사용하는 STL 과 기존 transformer block 의 차이점으로는 
local attention 과 shifted window mechanism 이 있다. self-attention 을 수행하기 위해 Input 을 정해진 window 에 따라 학습하는데, 이는 다음 그림과 같다.

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2-3.jpg" height="300">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure2 : Swin Transformer Input ]</figcaption>
</figure>

여기서 window 의 크기가 $M \times M$ 이므로 차원은 동일하게 유지한 채 윈도우의 크기로 높이와 너비를 나눠 차원을 다시 표현할 수 있다. 이후 윈도우 별로 self-attention 을 계산하고, 이를 수식으로 표현하면 다음과 같다.

$$
Q = XP_Q, \quad K = XP_K, \quad V = XP_V, \\
Attention(Q,K,V) = SoftMax(QK^T/\sqrt{d} + B)V
$$

여기서 $Q,K,V$ 는 각각 query, key, value 를 의미하고, $P_i \: for \: i\in \{Q,K,V\}$ 는 local window feature $X \in \mathbb{R}^{M^2 \times C}$ 에 대한 가중치다. 그리고 $B$ 는 상대적인 위치를 나타내기 위한 positional encoding 이다. 여기서 주목할 점은 다음과 같은 이유로 각 윈도우에서 사용하는 $P_i$ 가 모두 동일한 값을 사용한다는 것이다.

1. 파라미터 수 감소를 통해 모델을 더 간단하게 구성하고 효율적으로 구축
2. 윈도우 간 동일한 패턴을 학습함으로써 일관된 결과를 도출하도록 유도
3. 서로 다른 윈도우들 사이에서도 공통된 특징을 추출 및 처리



#### - Multi-Layer Perceptron
Multi-Layer Perceptron (MLP) 는 두 완전연결층(fully connected layer) 사이에 GELU(Gaussian Error Linear Unit) 를 추가한 형태다. STL 의 전체구조는 다음과 같다.

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2-1.jpg" height="200">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure3 : Swin Transformer Layer ]</figcaption>
</figure>

각 층을 수식으로 표현하면 다음과 같다.

$$
X = MSA(LN(X)) + X \\
X = MLP(LN(X)) + X
$$

만약 각 층마다 window 분할이 고정이면, window 간 정보가 독립적으로 처리되어 상호작용이 불가능하다. 따라서 window 분할 전 feature map 을 $\lfloor \frac{M}{2} \rfloor, \lfloor \frac{M}{2} \rfloor$ 만큼 이동해, local feature 를 학습하는 regular window 와 global feature 를 학습하는 shifted window 를 교대로 사용하며 층별 윈도우 간 상호작용을 하도록 한다. 


## 2.3. Residual Swin Transformer Block

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2-2.jpg" height="200">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure4 : Residual Swin Transformer Block (RSTB) ]</figcaption>
</figure>

Residual Swin Transformer Block (RSTB) 은 STL 을 쌓아 마지막에 합성곱을 적용한 Block 에 residual block 을 추가한 형태다. Block 의 마지막 부분에 합성곱을 추가한 이유는 다음과 같다.

1. local feature 학습
2. noise 제거를 통한 정제
3. 채널간 정보를 통합함으로써 상호작용 강화

또한, residual block 을 추가한 이유는 다음과 같다.

1. 기울기 소실 방지
2. 학습 안정성 및 효율성
3. 정보 전달 강화
4. 입력 데이터를 손실 없이 전달해 local 과 global feature 모두 학습


## 2.4. Shallow & Deep Feature Extraction
#### - Shallow Feature Extraction
Shallow Feature Extraction 은 입력 데이터에서 이미지의 기본적인 패턴, 엣지, 텍스쳐와 같은 단순하고 지역적인 정보를 학습하는 과정이다. 주어진 저해상도 이미지 $I_L \in \mathbb{R}^{H\times W \times C}$ 에 대해 $3\times3$ 합성곱 층을 적용한다. 합성곱 층은 안정적인 최적화와 더 좋은 결과를 내기 때문에 초기 이미지 처리에 적합하다. shallow feature $F_0$ 에 대해 다음과 같이 표현할 수 있다.

$$
F_0 = H_{SF} (I_{LQ})
$$

#### - Deep Feature Extraction
Deep Feature Extraction 은 이미지의 전체적인 관계, 문맥적 정보, 구조적 패턴을 학습하는 과정이다. 이는 shallow feature $F_0$ 를 입력으로 받아 RSTB 와 $3\times3$ 합성곱 층으로 구성된 깊은 신경망을 통과해 계산을 진행한다. 즉, local 과 global feature 를 합쳐 이미지를 학습시키는 과정이다. 이를 수식으로 표현하면 다음과 같다.

$$
F_i = H_{RSTB_i} (F_{i-1}), \quad i = 1, 2, ... , K \\
F_{DF} = H_{Conv} (F_K)
$$

## 2.5. High Quality Image Reconstruction
고해상도로 이미지를 복원하기 위해 shallow feature 와 deep feature 를 합쳐 복원 모듈에 통과시킨다. 여기서 복원 모듈은 일종의 decoder 로 task 마다 다른 모듈을 적용한다. Deep Feature Extraction 에 대한 장거리 skip connection 을 이용해 복원모듈에 shallow feature 를 전달할 수 있는데, 이는 deep feature extraction 모듈이 deep feature 에 집중해 안정적으로 학습할 수 있도록 한다. 만약 업샘플링이 필요한 경우 sub-pixel convolution layer 를 적용한다. 이를 다음과 같이 표현할 수 있다.

$$
\hat{I}_{HQ} = H_{REC} (F_0 + F_{DF})
$$

만약 업샘플링이 필요하지 않으면, 단일 합성곱층을 이용해 이미지를 복원한다. 왜냐하면 복잡한 신경망 구조없이도 왜곡을 제거하거나 세부 정보를 복원할 수 있기 때문이다. 추가적으로 skip connection 을 사용해 저해상도와 고해상도 이미지 간 잔차를 복원한다. 이는 다음과 같이 표현할 수 있다.

$$
\hat{I}_HQ = H_{SwinIR}(I_{LQ}) + I_LQ
$$

## 2.6. Loss Function
#### - Image Super Resolution
초해상화 작업을 수행하는 경우, 손실함수는 L1 픽셀 손실 함수를 사용한다. 이는 다음과 같다.

$$
\mathcal{L} = || \hat{I}_{HQ} - I_{HQ} ||_1
$$

전통적이거나 경량화 초해상화 작업에서는 제안한 신경망의 효율성을 보여주기 위해 단순한 L1 픽셀 손실만 사용한다. 반면, 실제 작업에서는 시각적 품질을 향상시키기 위해 픽셀 손실, GAN 손실, 지각적 손실의 조합을 사용한다. 

#### - Image Denoising & JPEG Compression Artifact Reduction
이미지 노이즈 제거와 JPEG 압축 아티팩트 감소에서는 Charbonnier 손실을 사용하며, 이는 다음과 같다.

$$
\mathcal{L} = \sqrt{||\hat{I}_{HQ} - I_{HQ}||^2 + \epsilon^2}
$$

여기서 $\epsilon$ 은 실험적으로 $10^{-3}$ 으로 설정된 상수다.





# 3. Experiments
## 3.1. Experimental Setup
각 작업에 대한 파라미터들은 다음과 같이 설정한다.

|    Task     | STL     | RSTB     | window size | # of Channel |  # Atttention Head |
|----------|----------|----------|--|-|-|
| Classical SR | 6 | 데이터 3 |||
| Real-World SR | 6 | 데이터 6 |||
| Lightweight SR|6||||
| Denoising|6||||
| JPEG Compression|||||



<table>
    <tr>
        <th>Task</th>
        <th>열 2</th>
        <th>열 3</th>
    </tr>
    <tr>
        <td>데이터 1</td>
        <td>데이터 2</td>
        <td>데이터 3</td>
    </tr>
    <tr>
        <td>데이터 4</td>
        <td>데이터 5</td>
        <td>데이터 6</td>
    </tr>
    <tr>
        <td>데이터 7</td>
        <td>데이터 8</td>
        <td>데이터 9</td>
    </tr>
</table>

<table>
    <tr>
        <th>행 번호</th>
        <th>열 1</th>
        <th>열 2</th>
        <th>열 3</th>
    </tr>
    <tr>
        <td rowspan="2">1</td>
        <td>데이터 1</td>
        <td>데이터 2</td>
        <td>데이터 3</td>
    </tr>
    <tr>
        <td colspan="2">데이터 4와 5 병합</td>
        <td>데이터 6</td>
    </tr>
    <tr>
        <td>3</td>
        <td>데이터 7</td>
        <td>데이터 8</td>
        <td>데이터 9</td>
    </tr>
</table>





<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/settings_IC.jpg" height = 100>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 2 : Hyperparameters Settings for Image Classification ]</figcaption>
</figure>

train dataset으로 IamgeNet-1K을 이용했다. settings2에서 fine-tuning 진행시 30epoch, batch size 1024, constant learning rate $10^{-5}$, weight decay $10^{-8}$ 로 진행하였다.

- #### Results on Setting1

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/result1.jpg" height = 400>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 3 : Comparison 1 of different backbones on ImageNet-1K ]</figcaption>
</figure>

복잡도가 유사한 DeiT에 대해 Swin이 더 좋은 성능을 보인다. 또한 ConvNets 과 비교했을 때도 Swin이 조금 더 좋은 성능과 속도를 보인다. 

- #### Results on Setting2

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/result2.jpg" height = 200>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 4 : Comparison 2 of different backbones on ImageNet-1K ]</figcaption>
</figure>

다른 모델들에 비해 Swin이 조금 더 좋은 성능과 속도를 보인다.

## 3.2. Object Detection
- #### Settings

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/settings_OD.jpg" height = 150>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 5 : Hyperparameters Settings for Object Detection on COCO ]</figcaption>
</figure>

COCO 2017로 학습을 진행하였다. 시스템 수준 비교를 위해 instaboost, 강력한 다중 스케일 학습, 6배 스케줄 (epochs 72), soft-NMS, 그리고 ImageNet-22K 사전 학습 모델을 초기화로 사용하는 개선된 HTC (HTC++)를 사용한다.

- #### Results on Various Frameworks

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/result3.jpg" height = 200>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 6 : Result 1 on COCO Object Detection ]</figcaption>
</figure>

4가지 프레임워크에서 모두 기존 ResNe(X)t-50 보다 parameter 수가 많고 FLOPS는 비슷하지만, 이에 반해 높은 정확도을 보인다. 

- #### Results on Various backbones with cascade Mask R-CNN

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/result4.jpg" height = 200>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 7 : Result 2 on COCO Object Detection ]</figcaption>
</figure>


또한 Cascade Mask R-CNN을 backbone network로 이용했을 때 역시 다른 모델들에 비해 Swin이 높은 정확도를 보인다. 이 때 infernce speed에 대해, ResNe(X)t는 highly optimized Cudnn 함수를 이용했지만 Swin은 PyTorch 함수를 이용하였기에 최적화 성능에서 어느 정도 차이가 발생하였다. 

- #### Results on System-level Comparsion

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/result5.jpg" height = 300>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 8 : Result 3 on COCO Object Detection ]</figcaption>
</figure>

validation set에 대한 Average pooling 값과 FLOPs 를 비교한 표다. 기존 SOTA 모델들과 비교했을 때, Swin이 가장 우수한 성능을 보인다.

## 3.3. Semantic Segmentation

#### - Settings
ADE20K 로 학습을 진행하였다. base framework로는 mmseg에서 UperNet을 이용하였다. 

#### - Results 

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/result6.jpg" height = 300>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 9 : Results of Semantic Segmentation on ADE20K ]</figcaption>
</figure>

평가지표로 mIOU를 이용하였다. 다른 모델들과 비교했을 때, Swin이 validation set과 test set에서 모두 가장 높은 점수를 보인다.

## 3.4. Ablation Study
#### - Results on Relative Position Bias
3.1 ~ 3.3 에서 수행한 작업들에 대해 Ablation Study를 진행하였다. 

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/ablation1.jpg" height = 200>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 10 : Results of Ablation study ]</figcaption>
</figure>

1. Shifted Windows

표의 상단은 shifted windows에 대한 성능을 비교한 결과를 보인다. 연속되는 층들에 대해 window간 연결을 할 때 shifted windows를 이용하는 것이 더 효율적이라는 것을 알 수 있다.

2. Relative Position Bias

표의 하단은 다양한 위치 임베딩 접근 방식을 비교한 결과를 보인다. Swin-T에서 상대 위치 편향을 사용했기 때문에 더 좋은 효과를 보인다. 또한 절대 위치 임베딩의 포함 여부가 이미지 분류 정확도를 향상시키지만, 객체 탐지 및 시맨틱 세그멘테이션 성능은 오히려 저하된다.



#### - Results on Shifted Winodws and Different self-attention methods

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/ablation2.jpg" height = 150>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 11 : Results of Ablation study 2 ]</figcaption>
</figure>


위 표는 여러 self-attention 계산 방법을 비교한 것이다. 본 논문에서 제시하는 Cyclic Shift는 더 깊은 층에서의 naive padding보다 더 좋은 성능을 보인다.

또한 4 단계에 걸친 MSA에서 shifted window가 sliding window에 비해 대부분 더 좋은 성능을 보인다. 이를 바탕으로 각각 Image Classification, Object Detection, Semantic Segmentation에 적용한 결과는 다음 표와 같다.

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinTransformer/ablation3.jpg" height = 100>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 12 : Results of Ablation study 3 ]</figcaption>
</figure>


# 4. Code


# 참고
https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows