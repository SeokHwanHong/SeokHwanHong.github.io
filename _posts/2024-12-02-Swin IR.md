---
layout: single        # 문서 형식
title: Swin IR Image Restoration Using Swin Transformer (2021)        # 제목
categories: SemanticSegmentation   # 카테고리
tag: [DL, Image, Transformer]
sidebar_main : true # 좌측 글 목록

author_profile: true  # 사이드바 visible 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "counts"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
Swin Transformer, Shifted Window, Shallow Feature Extraction, Deep Feature Extraction, Image Reconstruction


# 1. Introduction
## 1.1. CNN based Methods 
대부분의 CNN 기반 방법들은 잔차 학습, dense connection 등과 같은 구조들로 구성되는데, 일반적으로 합성곱으로 기인하는 문제점들이 발생한다. 

- 문제점 1
이미지와 합성곱 커널간 상호작용이 이미지의 내용들과 무관, 모든 이미지 영역에서 동일하게 동작

- 문제점 2
주변 픽셀들과의 연관성은 잘 처리하지만, 멀리 떨어져 있는 픽셀과의 관계를 효과적으로 다루지는 못함.


## 1.2. Transformer based Methods
CNN 의 단점들을 보완하고자 이미지 분야에도 transformer 를 도입해, 이미지의 전체적인 특징을 학습하고자 하였다. 또한 이미지 복원에서 ViT 를 사용할 때, 입력 이미지를 고정된 크기의 여러 패치들로 나눠 독립적으로 처리한다. 하지만 이 역시 다음과 같은 문제점들이 발생한다.

- 문제점 1

패치의 경계부분의 픽셀들은 패치 바깥의 픽셀들과 상호작용이 불가능

- 문제점 2

복원된 이미지에서 패치 사이 경계선에 인공적인 흔적이 발생할 가능성 존재

따라서 본 논문에서는 CNN 기반 방법의 장점인 지역적 특성을 잘 반영하는 것과 transformer 기반의 장점인 shifted window 를 이용한 멀리 떨어져 있는 픽셀과의 관계를 잘 학습한다는 것을 융합하는 Swin Transformer 를 사용해 이미지를 복원하고자 한다.



# 2.Methods
## 2.1. Overall Architecture

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2.jpg" height="225">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 1 : The Architecture of the Proposed SwinIR for Image Restoration ]</figcaption>
</figure>

Shallow 와 Deep feature extraction 모듈은 은 항상 동일하게 구성하지만, taask 에 따라 reconstruction 모듈은 다르게 구성한다.

## 2.2. Swin Transformer Layer

Swin Transformer Layer (STL) 는 기존 transformer 층의 기본적인 multi-head self-attention 에 기반한 층이다. 전체 구조를 확인하면 STL을 쌓아 Residual Swin Transformer Block (RSTB) 를 구성하고, RSTB 를 쌓아 Depp Feature Extraction 을 수행한다.


#### - Self-Attention in Local Window
본 논문에서 사용하는 STL 과 기존 transformer block 의 차이점으로는 
local attention 과 shifted window mechanism 이 있다. self-attention 을 수행하기 위해 Input 을 정해진 window 에 따라 학습하는데, 이는 다음 그림과 같다.

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2-3.jpg" height="300">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 2 : Swin Transformer Input ]</figcaption>
</figure>

여기서 window 의 크기가 $M \times M$ 이므로 차원은 동일하게 유지한 채 윈도우의 크기로 높이와 너비를 나눠 차원을 다시 표현할 수 있다. 이후 윈도우 별로 self-attention 을 계산하고, 이를 수식으로 표현하면 다음과 같다.

$$
\begin{split}
    &Q = XP_Q, \quad K = XP_K, \quad V = XP_V, \\
    Atten&tion(Q,K,V) = SoftMax(QK^T/\sqrt{d} + B) V
\end{split}
$$

여기서 $Q,K,V$ 는 각각 query, key, value 를 의미하고, $P_i \: for \: i\in \{Q,K,V\}$ 는 local window feature $X \in \mathbb{R}^{M^2 \times C}$ 에 대한 가중치다. 그리고 $B$ 는 상대적인 위치를 나타내기 위한 positional encoding 이다. 여기서 주목할 점은 다음과 같은 이유로 각 윈도우에서 사용하는 $P_i$ 가 모두 동일한 값을 사용한다는 것이다.

1. 파라미터 수 감소를 통해 모델을 더 간단하게 구성하고 효율적으로 구축
2. 윈도우 간 동일한 패턴을 학습함으로써 일관된 결과를 도출하도록 유도
3. 서로 다른 윈도우들 사이에서도 공통된 특징을 추출 및 처리



#### - Multi-Layer Perceptron
Multi-Layer Perceptron (MLP) 는 두 완전연결층(fully connected layer) 사이에 GELU(Gaussian Error Linear Unit) 를 추가한 형태다. STL 의 전체구조는 다음과 같다.

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2-1.jpg" height="200">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 3 : Swin Transformer Layer ]</figcaption>
</figure>

각 층을 수식으로 표현하면 다음과 같다.

$$
\begin{split}
X = MSA(LN(X)) + X \\
X = MLP(LN(X)) + X
\end{split}
$$

만약 각 층마다 window 분할이 고정이면, window 간 정보가 독립적으로 처리되어 상호작용이 불가능하다. 따라서 window 분할 전 feature map 을 $\lfloor \frac{M}{2} \rfloor, \lfloor \frac{M}{2} \rfloor$ 만큼 이동해, local feature 를 학습하는 regular window 와 global feature 를 학습하는 shifted window 를 교대로 사용하며 층별 윈도우 간 상호작용을 하도록 한다. 


## 2.3. Residual Swin Transformer Block

<figure style="text-align: center; display: inline-block; width: 100%;">
  <img src="/images/SwinIR/figure2-2.jpg" height="200">
  <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 4 : Residual Swin Transformer Block (RSTB) ]</figcaption>
</figure>

Residual Swin Transformer Block (RSTB) 은 STL 을 쌓아 마지막에 합성곱을 적용한 Block 에 residual block 을 추가한 형태다. Block 의 마지막 부분에 합성곱을 추가한 이유는 다음과 같다.

1. local feature 학습
2. noise 제거를 통한 정제
3. 채널간 정보를 통합함으로써 상호작용 강화

또한, residual block 을 추가한 이유는 다음과 같다.

1. 기울기 소실 방지
2. 학습 안정성 및 효율성
3. 정보 전달 강화
4. 입력 데이터를 손실 없이 전달해 local 과 global feature 모두 학습


## 2.4. Shallow & Deep Feature Extraction
#### - Shallow Feature Extraction
Shallow Feature Extraction 은 입력 데이터에서 이미지의 기본적인 패턴, 엣지, 텍스쳐와 같은 단순하고 지역적인 정보를 학습하는 과정이다. 주어진 저해상도 이미지 $I_L \in \mathbb{R}^{H\times W \times C}$ 에 대해 $3\times3$ 합성곱 층을 적용한다. 합성곱 층은 안정적인 최적화와 더 좋은 결과를 내기 때문에 초기 이미지 처리에 적합하다. shallow feature $F_0$ 에 대해 다음과 같이 표현할 수 있다.

$$
F_0 = H_{SF} (I_{LQ})
$$

#### - Deep Feature Extraction
Deep Feature Extraction 은 이미지의 전체적인 관계, 문맥적 정보, 구조적 패턴을 학습하는 과정이다. 이는 shallow feature $F_0$ 를 입력으로 받아 RSTB 와 $3\times3$ 합성곱 층으로 구성된 깊은 신경망을 통과해 계산을 진행한다. 즉, local 과 global feature 를 합쳐 이미지를 학습시키는 과정이다. 이를 수식으로 표현하면 다음과 같다.

$$
\begin{split}
F_i &= H_{RSTB_i} (F_{i-1}), \quad i = 1, 2, ... , K \\
&F_{DF} = H_{Conv} (F_K)
\end{split}
$$

## 2.5. High Quality Image Reconstruction
고해상도로 이미지를 복원하기 위해 shallow feature 와 deep feature 를 합쳐 복원 모듈에 통과시킨다. 여기서 복원 모듈은 일종의 decoder 로 task 마다 다른 모듈을 적용한다. Deep Feature Extraction 에 대한 장거리 skip connection 을 이용해 복원모듈에 shallow feature 를 전달할 수 있는데, 이는 deep feature extraction 모듈이 deep feature 에 집중해 안정적으로 학습할 수 있도록 한다. 만약 업샘플링이 필요한 경우 sub-pixel convolution layer 를 적용한다. 이를 다음과 같이 표현할 수 있다.

$$
\hat{I}_{HQ} = H_{REC} (F_0 + F_{DF})
$$

만약 업샘플링이 필요하지 않으면, 단일 합성곱층을 이용해 이미지를 복원한다. 왜냐하면 복잡한 신경망 구조없이도 왜곡을 제거하거나 세부 정보를 복원할 수 있기 때문이다. 추가적으로 skip connection 을 사용해 저해상도와 고해상도 이미지 간 잔차를 복원한다. 이는 다음과 같이 표현할 수 있다.

{% raw %}
$$
\hat{I}_HQ = H_{SwinIR}(I_{LQ}) + I_LQ
$$
{% endraw %}



## 2.6. Loss Function
#### - Image Super Resolution
초해상화 작업을 수행하는 경우, 손실함수는 L1 픽셀 손실 함수를 사용한다. 이는 다음과 같다.

$$
\mathcal{L} = || \hat{I}_{HQ} - I_{HQ} ||_1
$$

전통적이거나 경량화 초해상화 작업에서는 제안한 신경망의 효율성을 보여주기 위해 단순한 L1 픽셀 손실만 사용한다. 반면, 실제 작업에서는 시각적 품질을 향상시키기 위해 픽셀 손실, GAN 손실, 지각적 손실의 조합을 사용한다. 

#### - Image Denoising & JPEG Compression Artifact Reduction
이미지 노이즈 제거와 JPEG 압축 아티팩트 감소에서는 Charbonnier 손실을 사용하며, 이는 다음과 같다.

$$
\mathcal{L} = \sqrt{||\hat{I}_{HQ} - I_{HQ}||^2 + \epsilon^2}
$$

여기서 $\epsilon$ 은 실험적으로 $10^{-3}$ 으로 설정된 상수다.



# 3. Experiments
## 3.1. Experimental Setup
각 작업에 대한 파라미터들은 다음과 같이 설정한다.

| Task | STL | RSTB | window size | Channel | Atttention Head |
|----------|----------|----------|----------|----------|----------|
| Classical SR | 6 | 6 | 8 | 180 | 6 | 
| Real-World SR | 6 | 6 | 8 | 180 | 6 |
| Lightweight SR | 6 | 4 | 8 | 60 | 6 |
| Denoising | 6 | 6 | 8 | 180 | 6 |
| JPEG Compression | 6 | 6 | 7 | 180 | 6 |

JPEG 압축 에서 window size 가 7인 이유는 window size 가 8인 경우부터 성능이 급격하게 떨어지는데, 이는 JPEG 의 인코딩이 $8 \times 8$ 로 이미지를 분할하기 때문이다.



## 3.2. Ablation Study and Discussion
ablation study 를 위해 2배 SR 에 대해 DIV2K 로 SwinIR 을 학습해 Manga109 로 test를 진행하였다. 

#### - Impact of channel number, RSTB number, STL number

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/figure3-1.jpg" height = 150>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 5 : Ablation Study 1 on different settings of SwinIR ]</figcaption>
</figure>

각 case에 대해 PSNR 을 확인해본 결과, PSNR 은 하이퍼파라미터 3개 (Channel, STL, RSTB) 와 양의 상관관계를 갖는다. Channel 을 계속 증가시키면서 PSNR 성능이 증가하지만, 특정 값으로 수렴하게 될 것으로 예상된다. 따라서 성능과 모델 크기 간 균형을 조정하기 위해 나머지 실험들에서는 Channel 의 수를 180으로 설정한다.


#### - Impact of patch size and training image number; Model convergence comparison
transformer 모델과 cnn 모델을 비교하기 위해 대조군으로 RCAN 을 비교한다. 

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/figure3-2.jpg" height = 150>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 6 : Ablation Study 2 on different settings of SwinIR ]</figcaption>
</figure>

patch size, percentage of used images, training iteration 등 3가지 항목에서 SwinIR 이 RCAN 보다 좋은 성능을 보임을 확인할 수 있다. 또한, 기존 transformer 모델들은 대규모 데이터셋에서의 훈련을 통해 좋은 성능을 보이지만, SwinIR 은 적은 데이터셋에서도 좋은 성능을 보인다. 그리고 SwinIR 이 RCAN 보다 수렴속도가 빠른 것 역시 확인가능하다. 

#### - Impact of Residual connection and Convolution later in RSTB

| Design | No Residual | 1 $\times$ 1 Conv. | 3 $\times$ 3 Conv. | Three 3 $\times$ 3 Conv. | 
|----------|----------|----------|----------|----------|
| PSNR | 39.42 | 39.45 | 39.58 | 39.56 | 

잔차가 없는 경우와 있는 경우에 따라 성능이 달라짐을 알 수 있다. 또한, 3 $\times$ 3 합성곱을 사용한 경우에는 주변 픽셀값들에 대한 정보를 추출할 수 있는 반면, 1 $\times$ 1 합성곱을 사용한 경우는 그럴 수 없기 때문에 상대적으로 성능이 더 낮다. 그리고 3 $\times$ 3 합성곱을 3번 사용한 경우, 파라미터의 수는 더 감소하였지만 3 $\times$ 3 합성곱을 사용한 경우보다 성능이 낮은 것을 확인할 수 있다.



## 3.3. Results on Image SR

#### - Classical Image SR
<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/table2.jpg" height = 400>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 1 : Quantitative Comparison for Classical Image SR ]</figcaption>
</figure>

#### - Lightweight Image SR
<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/table3.jpg" height = 200>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 2 : Quantitative Comparison for Lightweight Image SR ]</figcaption>
</figure>

#### - Real-world Image SR
<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/figure5.jpg" height = 250>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 7 : Vosual Comparison of Real-World Image SR ]</figcaption>
</figure>

실세계의 이미지 SR 에 대한 SwinIR 의 성능을 확인하기 위해, 저해상도 이미지 생성을 위해 BSRGAN 을 사용함으로써 SwinIR 을 재학습하였다. 이러한 방식의 학습은 모델이 일반화된 성능을 발휘할 수 있도록 한다.또한, 실제 정답 이미지가 없기 때문에 bicubic 모델인 ESRGAN 과 다른 모델들을 비교한다. 


#### - Results on JPEG Compression Artifact Reduction
<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/table4.jpg" height = 100>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 3 : Quantitative Comparison for JPEG Compression Artifact Reduction ]</figcaption>
</figure>


#### - Results on Image Denoising
1. Grayscale Image
<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/figure6.jpg" height = 125>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 8 : Visual Comparison for Grayscale Image Denoising ]</figcaption>
</figure>

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/table5.jpg" height = 125>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 4 : Quatitative Comparison for Grayscale Image Denoising ]</figcaption>
</figure>


2. Color Image
<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/figure7.jpg" height = 125>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 9 : Visual Comparison for Grayscale Image Denoising ]</figcaption>
</figure>

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/SwinIR/table6.jpg" height = 150>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table 5 : Quantitative Comparison for Color Image Denoising ]</figcaption>
</figure>


# 4. Code


# 참고
https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows