---
layout: single        # 문서 형식
title: Image Restoration Using Joint Statistical Modeling in Space-Transform Domain (2014) # 제목
categories: Electronics    # 카테고리
tag: [CV, Mathematics, Electronics, Statistics]
author_profile: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "counts"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
Joint Statistics, Local Statistics, Nonlocal Statistics



# 1. Proposed Joint Statistical Modeling In Space-Transform Domain
## 1.1. Natural Images
#### - Definition
자연 이미지 (Natural Image) 란 일반적으로 자연에서 관찰되는 장면이나 객체를 담은 이미지이다. 이는 풍경, 인물 사진 등 다양한 현실 세계의 모습을 표현한다.


<p align="center">
  <a href="#">
    <img src="/images/JSM/figure1.jpg" height="300" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 1 : Illustrations for Local Smoothness and Local Self-similarity of Natural Images ]</b> 
</p>


#### - Properties
**1. 비정형성과 복잡성**

정형화된 패턴이 적고 예측하기 어려운 비정형적 요소가 많이 포함


**2. 자연스러운 색 분포**

특정 색상이 두드러지기보다 여러 색상이 자연스럽게 조화를 이룸


**3. 통계적 자기 유사성**

특정 패턴이나 질감이 다양한 공간적 스케일에서 유사하게 반복


**4. 노이즈와 변동성**

빛, 날씨, 카메라 센서의 민감도 등 다양한 외부 요인으로 인해 노이즈가 포함


## 1.2. Joint Statistical Modeling
본 논문에서는 기존 regularized inverse problem 의 정규화 텀을 바꿔 다음과 정의한다.

$$
\begin{split}
    \Psi_{JSM}(u) = \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM}(u)
\end{split}
$$

여기서 $$\Psi_{LSM}$$ 은 지역적 부드러움에 대한 사전분포, $$\Psi_{NLSM}$$ 은 비지역적 자기 유사성을 반영하는 사전분포이다. 또한, $$\tau$$ 와 $$\lambda$$ 는 각각  $$\Psi_{LSM}$$ 와 $$\Psi_{NLSM}$$ 을 조절하는 파라미터이다.



## 1.3. Local Statistical Modeling for Smoothness in Space Domain
지역적 부드러움(local smoothness) 는 2차원 공간 영역에서 인접한 픽셀의 유사성을 의미한다. 다음 figure2 의 (a) 는 고주파 대역 필터를 통과한 이미지를 나타내고, (b) 는 주파수 영역에서 주파수의 분포를 나타낸다.

<p align="center">
  <a href="#">
    <img src="/images/JSM/figure2.jpg" height="300" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 2 : Illustrations for LSM for Smoothness in Space Domain at Pixel Level ]</b> 
</p>


#### - Vertical & Horizontal Filter

$$
\begin{split}
    \mathcal{D}_v = [1 -1]^T, \mathcal{D}_h = [1 -1], 
\end{split}
$$

수직 및 수평 필터는 이웃한 픽셀이 유사하지 않은 경우를 표현한다. 따라서 이 경우 픽셀의 분포는 figure2 (b) 와 같이 나타나며 대부분의 값들이 0 에 모여있는 것을 확인할 수 있다.


#### - Generalized Gaussian Distribution (GGD)
수직 및 수평 필터의 주변 확률 분포는 보통 GGD 로 표현되고, 이는 다음과 같이 정의된다.

$$
\begin{split}
    p_{GGD} = \frac{\mathbf{v} \cdot \eta(v)}{2 \cdot \Gamma(1/\mathbf{v})} \cdot \frac{1}{\sigma_\mathbf{x}} \cdot \exp \left[-({ \frac{\eta(\mathbf{v}) \cdot \vert \mathbf{x} \vert}{\sigma_\mathbf{x}}} ) ^\mathbf{v} \right]
\end{split}
$$

여기서 $$\eta(\mathbf{v}) = \sqrt{\Gamma(3/\mathbf{v}){\Gamma(1/\mathbf{v})}}, \; \Gamma(t) = \int_0^{\infty} e^{-u}u^{t-1} du $$ 는 감마 함수, $$\sigma_x$$ 는 표준편차, $$\mathbf{v}$$ 는 분포의 형태를 결정하는 parameter 이다. $$\mathbf{v}$$ 값에 따른 분포는 다음과 같다.  

$$
\begin{split}
    p_{GGD} (\mathbf{X}) = \begin{cases} 
    \text{Gaussian Dist'n} & \text{if} \quad \mathbf{v} = 2 \\ 
    \text{Laplacian Dist'n} & \text{if} \quad \mathbf{v} = 1 \\
    \text{hyper-Laplacian} & \text{if} \quad 0 < \mathbf{v} < 1 \\ 
    \end{cases} \\
\end{split}
$$

본 논문에서는 이미지의 통계량에 대한 정확한 묘사와 최적화 문제 간 균형을 맞추기 위해 $$\mathbf{v} = 1$$ 인 Laplacian 분포를 적용한다. 따라서 LSM 을 다음과 같이 정의한다.

$$
\begin{split}
    \Psi_{LSM}(u) = \Vert \mathcal{D}u \Vert_{1} = \Vert \mathcal{D}_v u \Vert_{1}  + \Vert \mathcal{D}_h u \Vert_{1}
\end{split}
$$

이는 anisotropic TV 와 유사한 형태이며 LSM 이 이미지 내 부드러움을 강조하는데만 사용된다는 것을 의미한다. 그리고 볼록 최적화와 계산 복잡성이 낮다는 장점을 갖는다. 


## 1.4. Nonlocal Statistical Modeling for Self-Similarity in Transform Domain
Nonlocal Statistical Modeling (NLSM) 은 LSM 과 다르게 자연이미지의 비국소적 부분의 반복적인 특징을 나타낸다. 그리고 이미지의 깔끔함과 엣지를 유지하는데 사용되기 때문에 비국소적 특징 보존에 효과적이다. 

#### - NLSM Process
NLSM 을 구성하는 과정은 다음 그림과 같다.

<p align="center">
  <a href="#">
    <img src="/images/JSM/figure3.jpg" height="130" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 3 : Illustrations for NLSM for Self-similarity in 3-D Transform Domain at Block Level ]</b> 
</p>

step 1. 크기가 $$N$$ 인 이미지를 $$n$$ 개의 겹치는 블록 $$u^i$$ 로 나눈다. 여기서 각 블록 $$u^i$$ 는 크기가 $$b_s$$ 이다. 

step 2. 크기가 $$L \times L$$ 인 윈도우 내 블록 $$u^i$$ 중 가장 유사한 블록 $$C$$ 개를 탐색한다. 여기서 $$C$$ 를 고정된 숫자로 설정하며 유클리드 거리로 유사성을 측정한다. 유사한 블럭 집합 $$S_{u^i}$$ 을 다음과 같이 정의한다.

$$
\begin{split}
    S_{u^i} = \{ S_{u^i \otimes 1}, ... , S_{u^i \otimes c} \} 
\end{split}
$$  

step 3. 각 $$S_{u^i}$$ 에 대해, $$S_{u^i}$$ 에 속하는 $$C$$ 개의 블록을 3차원 배열로 쌓아 $$Z_{u^i}$$ 로 나타낸다.

step 4. 3차원 변환 연산자 $$T^{3D}$$ 에 대해, $$T^{3D}(Z_{u^i})$$ 를 $$Z_{u^i}$$ 의 변환 계수로 정의한다. 그리고 $$\Theta_u$$ 는 모든 변환 계수에 대한 열벡터이며, 크기가 $$K = b_s*c*n $$ 인 이미지 $$u$$ 에 맞추어 $$T^{3D}(Z_{u^i})$$ 를 사전식 순서로 정렬해 구성한다. 

step 5. Figure3 의 가장 오른쪽 그림처럼 변환 계수의 히스토그램을 분석한다. 0을 중심으로 매우 날카로운 것을 확인할 수 있으며, LSM과 유사하게 GGD로 특징을 표현할 수 있다. 따라서 LSM 과 유사한 이유로 $$\Theta_u$$ 의 분포를 Laplacian 함수로 모델링한다. 그리고 NLSM 을 다음과 같이 정의한다.

$$
\begin{split}
    \Psi_{NLSM} (u) = \Vert \Theta_u \Vert _{1} = \sum_{i=1}^n \Vert T^{3D}(Z_{u^i}) \Vert _{1}
\end{split}
$$

#### - Inverse Operator $\Omega_{NLSM}$
1. $$\Theta_u$$ 를 계산 후, $$n$$ 개의 3차원 변환계수로 구성된 3차원 배열로 분할

2. 이를 역변환해 3차원 배열의 각 블록에 대한 추정치 생성

3. 각 블록 단위로 추정된 값은 원래 위치로 반환되고, 모든 블록 단위 추정치에 대한 평균을 계산해 최종 이미지의 추정치를 계산. 


## 1.5. Summary
따라서 JSM은 다음과 같이 정의할 수 있다.

$$
\begin{split}
    \Psi_{JSM}(u) = \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM}(u) = \tau \cdot \Vert \mathcal{D} u \Vert _{1} + \lambda \cdot \Vert \Theta_u \Vert _{1}
\end{split}
$$


# 2. Split Bregman based Iterative Algorithm for Image Restoration using JSM
## 2.1. Regularized Inverse Problem 
이미지 복원을 위해 새롭게 정의한 정규화 역문제는 다음과 같다.

$$
\begin{split}
    argmin_u \frac{1}{2} \Vert Hy-u \Vert _{2}^2 + \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM} (u)
\end{split}
$$


## 2.2. Split Bregman Iteration
Split Bregman Iteration (SBI) 는 최소화 문제에서 $l_1$ norm 을 해결하기 위해 제안되었다. SBI 의 아이디어는 제약이 없는 최소화 문제에 제약을 추가해 변수를 분리함과 동시에 Bregman 반복을 진행하는 것이다. 

#### - SBI Algorithm
$$G \in \mathbb{R}^{M \times N}, \; f : \mathbb{R}^N \rightarrow \mathbb{R}, \;  g : \mathbb{R}^M \rightarrow \mathbb{R} $$ (적절한 닫힌 볼록함수) 에 대해 제약이 없는 최적화 문제 $$\min_{u \in \mathbb{R}^N} f(u) + g(Gu)$$ 를 해결하기 위한 SBI 는 다음과 같다.

**1. Set $$k = d^{(0)} = u^{(0)} = v^{(0)} = 0$$ and choose $$\mu > 0$$**

**2. Repeat**

$$
\begin{split}
    &u^{(k+1)} = argmin_u f(u) + \frac{\mu}{2} \Vert Gu-v^{(k)}-d^{(k)} \Vert_{2}^2 \\ 
    &v^{(k+1)} = argmin_v g(v) + \frac{\mu}{2} \Vert Gu^{(k+1)}-v-d^{(k)} \Vert_{2}^2 \\ 
    &d^{(k+1)} = d^{(k)} - (Gu^{(k+1)}-v^{(k)}) \\ 
    &k = k+1
\end{split}
$$

**3. Until stopping criterion is satisfied**

#### - Applying SBI Algorithm 
$$f(u) = \frac{1}{2} \Vert Hu-y \Vert_{2}^2, \; g(v) = g(Gu) = \Psi_{JSM}(u), d^{(k)} = \begin{bmatrix} b^{(k)} \\ c^{(k)} \end{bmatrix} \in \mathbb{R}^{2N}, b^{(k)} \& \; c^{(k)}\in \mathbb{R}^N $$ 에 대해 최적화 문제 $$ argmin_{u \in \mathbb{R}^v, v \in \mathbb{R}^{2N}} f(u) + g(v) \;\; s.t. \;\; Gu = v $$ 를 해결하기 위한 SBI 적용하면 각 변수를 다음과 같이 표현할 수 있다. 


**1. Calculating $u$**

$$
\begin{split}
    u^{(k+1)} &= argmin_u f(u) + \frac{\mu}{2} \Vert Gu-v^{(k)}-d^{(k)} \Vert_{2}^2 \\
    &= argmin_u \frac{1}{2} \vert Hu - y \vert_{2}^2 + \frac{\mu}{2} \left \| \begin{bmatrix} I \\ I \end{bmatrix} u - \begin{bmatrix} w^{(k)} \\ x^{(k)} \end{bmatrix} - \begin{bmatrix} b^{(k)} \\ c^{(k)} \end{bmatrix} \right\|_2^2 \\
    &= argmin_u + \frac{\mu}{2} \vert u + w^{(k)} + b^{(k)} \vert_{2}^2 + \vert u + x^{(k)} + c^{(k)} \vert_{2}^2 
\end{split}
$$


**2. Calculating $v$**

$v$ 에 대한 식을 정리하면 $w$ 와 $x$ 로 각각 분리가 가능하다.

$$
\begin{split}
    v^{(k+1)} &= \begin{bmatrix} w^{(k+1)} \\ x^{(k+1)} \end{bmatrix} \\
    &= argmin_{w,x} \tau \cdot \Psi_{LSM}(w) + \lambda \cdot \Psi_{NLSM} (x) + \frac{\mu}{2} \left\| \begin{bmatrix} I \\ I \end{bmatrix} u^{(k+1)} - \begin{bmatrix} w \\ x \end{bmatrix} - \begin{bmatrix} b^{(k)} \\ c^{(k)} \end{bmatrix} \right\|_2^2 \\
    &= argmin_{w,x} \tau \cdot \Psi_{LSM}(w) + \lambda \cdot \Psi_{NLSM} (x) + \frac{\mu}{2} \Vert u^{(k+1)}-w-b^{(k)} \Vert_{2}^2 + \frac{\mu}{2} \Vert u^{(k+1)}-x-c^{(k)} \Vert_{2}^{2} \\
    & \rightarrow \begin{cases} w^{(k+1)} &= argmin_w \tau \cdot \Psi_{LSM}(w) + \frac{\mu}{2} \vert u^{(k+1)}-w-b^{(k)} \vert_{2}^{2} \\ 
    x^{(k+1)} &= argmin_x \lambda \cdot \Psi_{NLSM} (x) + \frac{\mu}{2} \| u^{(k+1)} - x - c^{(k)} \|_2^2 \\ \end{cases}
\end{split}
$$


**3. Calculating $d$**

$d$ 에 대한 식을 정리하면 $b$ 와 $c$ 로 각각 분리가 가능하다.

$$
\begin{split}
    d^{(k+1)} &= \begin{bmatrix} b^{(k+1)} \\ c^{(k+1)} \end{bmatrix} - \left( \begin{bmatrix} I \\ I \end{bmatrix} u^{(k+1)} - \begin{bmatrix} w^{(k+1)} \\ x^{(k+1)} \end{bmatrix} \right) \\
    & \rightarrow \begin{cases} b^{(k+1)} &= b^{(k)} - (u^{(k+1)} - w^{(k+1)}) \\ 
    c^{(k+1)} &= c^{(k)} - (u^{(k+1)} - x^{(k+1)}) \\ \end{cases}
\end{split}
$$


**4. Theorem 1**

SBI 가 더 빠르게 수렴하기 위해 다음과 같은 정리를 이용한다.

$$
\begin{split}
    \text{The proposed algo&rithm described by Table I converges to a solution of} \\
    argmin_u &\frac{1}{2} \Vert Hy - u \Vert_{2}^2 + \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM} (u)
\end{split}
$$

proof) 
제안한 알고리즘이 SBI 의 한 예시라는 것은 자명하다. $$f(\cdot), \Psi_{LSM}(\cdot), \Psi_{NLSM}(\cdot)$$ 이 닫히고 적절하고 볼록한 형태이기 때문에, 제안한 알고리즘의 수렴은 다음과 같은 full column rank 행렬 $$G$$ 로 보장된다.

$$
\begin{split}
    G = \begin{bmatrix} I \\ I \end{bmatrix} \in \mathbb{R}^{2N \times N} \qquad \Box
\end{split}
$$ 


**5. Summary**

따라서 정리한 식들에 대해 알고리즘을 정리하면 다음과 같다.

Input : 관측한 이미지 $y$ & 선형 행렬 연산자 $H$

Initialization : $$k = b^{(0)} = c^{(0)} = w^{(0)} = x^{(0)} = 0 \text{ and  choose } \mu, \tau, \lambda$$

Repeat 

$$
\begin{split}
    &u = argmin_u \frac{1}{2} \vert Hu-y \vert_{2}^2 + \frac{\mu}{2} \vert u-w^{(k)}-b^{(k)} \vert_{2}^2 + \vert u-x^{(k)}-c^{(k)} \vert_{2}^2 \\
    &p^{(k)} = u^{(k+1)}-b^{(k)}; \quad \nu = \tau/\mu \\
    &w^{(k+1)} = prox_{\nu} (\Psi_{LSM})(p^{(k)})  \\
    &r^{(k)} = u^{(k+1)}-c^{(k)}; \quad \alpha = \lambda/\mu \\
    &x^{(k+1)} = prox_{\alpha} (\Psi_{NLSM})(r^{(k)}) \\
    &b^{(k+1)} = b^{(k)}-(u^{(k+1)}-w^{(k+1)}) \\
    &c^{(k+1)} = c^{(k)}-(u^{(k+1)}-x^{(k+1)}) \\
\end{split}
$$

Until stopping criterion is satisfied

Output : Final restored image $u$


## 2.3. Choosing Variables
#### - Choosing $u$

$$
\begin{split}
    u = argmin_u \frac{1}{2} \vert Hu-y \vert_{2}^2 + \frac{\mu_1}{2} \vert u-w-b \vert_{2}^2 + \frac{\mu_2}{2} \vert u+x+c \vert_{2}^2 
\end{split}
$$

이를 $u$ 에 대해 편미분 후 정리하면 다음과 같이 표현가능하다.

$$
\begin{split}
    u = (HH^T + \tilde{\mu} I)^{-1} \cdot q \quad \text{where} \quad  q = H^T y + \mu_1 (w+b) + \mu_2 (x+c) \quad  \text{and} \quad \tilde{\mu} = \mu_1 + \mu_2
\end{split}
$$


**1. Image Inpainting**

선형 연산자 $H$ 를 이진 대각행렬로 정의한다. 따라서 $u$ 를 다음과 같이 표현할 수 있다.

$$
\begin{split}
    H = &diag(1, ... , 1, 0, 1, ... , 1) \quad \text{and} \quad HH^T = I \\
    \rightarrow \quad &u = \frac{1}{\tilde{\mu}} ( I - \frac{1}{1+\tilde{\mu}} H^T H) \cdot q
\end{split}
$$

여기서 $H$ 의 대각행렬 중 1 은 픽셀이 존재하는 위치, 0은 픽셀이 존재하지 않는 위치를 나타낸다. 또한, 위 식은 역행렬연산이 필요없기 때문에 매우 효율적으로 계산이 가능하다.


**2. Image Deblurring**

선형 연산자 $H$ 를 순환 합성곱 행렬(deblurring filter)로 정의한다. 따라서 $u$ 를 다음과 같이 표현할 수 있다.

$$
\begin{split}
    H = &U^{-1}DU  \\
    \rightarrow \quad &u = (U^{-1}D^*DU + \tilde{\mu}U^{-1}U)^{-1} = U^{-1}(\vert D \vert^2 + \tilde{\mu}I)^{-1}U 
\end{split}
$$

여기서 U 는 2차원 DFT 행렬, D 는 H로 표현되는 합성곱 연산자의 DFT 계수를 나타내는 대각행렬, $(\cdot)^*$ 은 켤례 복소수를 의미한다. 실제로 $U^{-1}U$ 연산이 FFT 알고리즘에 의해 연산비용이 $O(N \log N)$ 으로 효율적인 연산이 가능하다.



#### - Choosing $w$
$$\vert Du \vert_{1} $$ 의 non-smoothness 를 해결하기 위해 Fast Iterative Shrinkage-Thresholidng Algorithm (FISTA) 를 사용한다.

**1. FISTA Algorithm**

$$f(x) = \frac{1}{2} \vert Hu - y \vert_{2}^2, g(x) $$ : 정규화 항인 최적화 문제 $$argmin_x (F(x) = f(x) + g(x))$$ 에 대해 다음 순서로 알고리즘을 적용한다.

$$
\begin{split}
    &1.\text{Initilaize} \quad \text{initial value} \; x_0, \quad \text{auxiliary variable} \; y_1 = x_0 \quad \text{acceleration parameter } \; t_1 = 1   \\
    &2. \text{Calculate} \quad x_{k+1} = prox_{\alpha, g}(y_k - \alpha \nabla f(y_k)) \quad \alpha : \text{step size or learning rate} \\
    &3. \text{Update} \qquad t_{k+1} = \frac{1+\sqrt{1+4t_k^2}}{2} \quad \text{and} \quad y_{k+1} = x_{k+1} + \frac{t_k - 1}{t_{k+1}} (x_{k+1} - x_k) \\
    & \text{Until Convergence} \\
\end{split}
$$


#### - Choosing $x$
$x$ 에 노이즈가 추가된 관측치인 $r$ 에 대해 $x$ 를 다음과 같이 표현할 수 있다.

$$
\begin{split}
    x &= prox_{\alpha} (\Psi_{NLSM}) (r) \\
    &= argmin_x \frac{1}{2} \vert x-r \vert_{2}^2 + \alpha \cdot \Psi_{NLSM}(x) \\
    &= argmin_x \frac{1}{2} \vert x-r \vert_{2}^2 + \alpha \cdot \vert \Theta_x \vert_{1}
\end{split}
$$

**1. Residual Distribution**

이미지의 잔차에 대한 분포를 확인하기 위해 $$e = x-r$$ 의 통계량을 관찰한다. 예시 이미지로 $$Butterfly$$ 컬러 이미지를 사용하였는데, 원본 이미지에 Gaussian blur kernel을 적용하였고 표준편차가 0.5 인 Gaussian white noise 를 추가하였다. $$r^{(k)} = u^{(k)} - c^{(k-1)}$$ 에 대해 $$e^{(k)} = x^{(k)} - r^{(k)} $$ 과 분산 $$ Var(e^{(k)}) = \frac{1}{N} \vert x^{(k)} - r^{(k)} \vert_{2}^2 $$ 의 히스토그램을 관찰하였다. 

<p align="center">
  <a href="#">
    <img src="/images/JSM/figure6.jpg" height="250" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 4 : Distribution of Residuals and its Varaiance ]</b> 
</p>

Figure 4 를 확인하면, 각 반복에서 잔차가 평균이 0이고 분산이 $$Var(e^{(k)})$$ 인 GGD 를 따르는 것과 유사한 것을 알 수 있다. 또한 각 반복의 잔차들이 서로 연관성이 없으며 독립이라는 것도 알 수 있다. 따라서 다음과 같은 정리를 통해 분포에 대한 가정을 하고자 한다. 

**2. Theorem 2**

$$x \& r \in \mathbb{R}^N, \; \Theta_x \& \Theta_r \in \mathbb{R}^K$$ 에 대해 잔차 $$e = x-r$$ 와 $$j=1,...,N$$ 에 대해 잔차 $$e$$ 의 각 원소를 $$e(j)$$ 로 정의한다. 이 때 $$e(j)$$ 는 서로 독립이고 평균이 0이고 분산이 $$\sigma^2$$ 인 분포를 따른다. 그러면 모든 $$\epsilon > 0$$ 에 대해 다음이 성립한다.

$$
\begin{split}
    \lim_{N \rightarrow infty, K \rightarrow infty} \mathbb{P} \left[ \left\| \frac{1}{N} \vert x-r \vert_{2}^2 - \frac{1}{N} \vert \Theta_x - \Theta_r \vert_{2}^2 < \epsilon \right\| \right] = 1
\end{split}
$$

proof)
$$e(j)$$ 가 독립이라는 가정으로 $$e(j)^2$$ 도 독립이다. 또한 이는 평균이 0이고 분산이 $$\sigma^2$$ 이다. 대수의 법칙에 의해 다음이 성립한다.

$$
\begin{split}
    &\lim_{N \rightarrow infty} \mathbb{P} \left[ \left\| \frac{1}{N} \sum_{j=1}^N e(j)^2 - \sigma^2 \ \right\| < \frac{\epsilon}{2} \right] = 1, \\
    &\lim_{N \rightarrow infty} \mathbb{P} \left[ \left\| \frac{1}{N} \vert x-r \vert_{2}^2 - \sigma^2 \ \right\| < \frac{\epsilon}{2} \right] = 1 \\
\end{split}
$$

이 때 $$\Theta_e$$ 의 정의에 의해, $$\Theta_e(j)$$ 는 서로 독립이고 평균이 0에 분산이 $$\sigma^2$$ 이다. 따라서 동일한 방법으로 $$\Theta_e(j)^2$$ 에 대해 다음이 성립한다.

$$
\begin{split}
    &\lim_{K \rightarrow infty} \mathbb{P} \left[ \left\| \frac{1}{K} \sum_{j=1}^K \Theta_e(j)^2 - \sigma^2 \ \right\| < \frac{\epsilon}{2} \right] = 1, \\
    &\lim_{K \rightarrow infty} \mathbb{P} \left[ \left\| \frac{1}{K} \vert \Theta_x - \Theta_r \vert_{2}^2 - \sigma^2 \ \right\| < \frac{\epsilon}{2} \right] = 1  \quad \Box \\
\end{split}
$$


**3. Conclusion**

Theorem 2 에 의해 다음이 성립한다.

$$
\begin{split}
    \frac{1}{N} \vert x^{(k)} - r^{(k)} \vert_{2}^2 = \frac{1}{K} \vert \Theta_x^{(k)} - \Theta_r^{(k)} \vert_{2}^2 
\end{split}
$$

이를 $x$ 에 대입하면 다음과 같은 식을 얻을 수 있다.

$$
\begin{split}
    argmin_x \frac{1}{2} \vert \Theta_x - \Theta_r \vert_{2}^2 + \frac{K\alpha}{N} \vert \Theta_x \vert_{1}
\end{split}
$$

이 때 미지의 변수 $$\Theta_x$$ 가 요소별로 분할이 가능하기 때문에 $$\Theta_x(j)$$ 은 다음과 같은 soft thresholding 을 이용해 독립적으로 계산할 수 있다.

$$
\begin{split}
    &\Theta_x = soft(\Theta_r, \sqrt{2\rho}) \quad \text{where} \quad j = 1, ... ,K , \; \rho = \frac{K\alpha}{N} \; \text{and} \\
    &\Theta_x (j) = sign(\Theta_r(j)) \cdot \max \left[ \vert \Theta_r(j) \vert - \sqrt(2\rho) \right] \\
    &= \begin{cases} \Theta_r(j) - \sqrt{2\rho} \quad \text{if} \quad \Theta_r(j) \in \mathbb{R} (\sqrt{2\rho}, infty) \\ \qquad \quad 0 \qquad \quad \text{if} \quad \Theta_r(j) \in \mathbb{R} (-\sqrt{2\rho}, -\sqrt{2\rho}) \\ \Theta_r(j) + \sqrt{2\rho} \quad \text{if} \quad  \Theta_r(j) \in \mathbb{R} (-infty ,-\sqrt{2\rho}) \end{cases}
\end{split}
$$

최종적으로 $x$ 를 다음과 같이 표현한다.

$$
\begin{split}
    x = \Omega_{NLSM} (\Theta_x) = \Omega_{NLSM} (soft(\Theta_r, \sqrt{2\rho}))
\end{split}
$$



## 2.4. Summary of Proposed Algorithm

Input : 관측된 이미지 $y$ 와 선형 행렬 연산자 $H$

Initialization : $k = b^{0} = c^{0} = w^{0} = x^{0} = 0, u^{0} = y$

Choose : $\tau, \lambda, \mu_1, \mu_2$

Repeat

$$
\begin{split}
    &Compute \quad u^{(k+1)} = \begin{cases} \frac{1}{\tilde{\mu}} \left( I - \frac{1}{1+\tilde{\mu}}H^TH \cdot q \right) \: \text{if} \; \text{Inpainting} \\ U^{-1} \left( \vert D \vert^2 + \tilde{\mu}I \right)^{-1} U \; \; \text{if} \; \text{Deblurring} \end{cases} \text{where} \; \; q = H^T y + \mu_1 (w+b) + \mu_2(w+c)\\
    &p^{(k)} = u^{(k+1)} - b^{(k)}; \quad \nu = \tau / \mu_1 \\
    &\text{Compute} \quad w^{(k+1)} \text{by FISTA} \\
    &r^{(k+1)} = u^{(k+1)} - c^{(k)}; \quad \alpha = \lambda / \mu_2 \\
    &\text{Compute} \quad x^{(k+1)} = \Omega_{NLSM}(soft(\Theta_r, \sqrt{2\rho})) \\
    &b^{(k+1)} = b^{(k)} - (u^{(k+1)} - w^{(k+1)}) \\
    &c^{(k+1)} = c^{(k)} - (u^{(k+1)} - x^{(k+1)}) \\
\end{split}
$$

Until 지정한 반복횟수

Output : 최종 복원한 이미지 $u$