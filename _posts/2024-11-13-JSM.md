---
layout: single        # 문서 형식
title: Image Restoration Using Joint Statistical Modeling in Space-Transform Domain (2014) # 제목
categories: Electronics    # 카테고리
tag: [CV, Mathematics, Electronics, Statistics]
toc: true             # 글 목차
toc_sticky : true
toc_label: 목차
author_profile: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
Joint Statistics, Local Statistics, Nonlocal Statistics



# 1. Proposed Joint Statistical Modeling In Space-Transform Domain
## 1.1. Natural Images
#### - Definition
자연 이미지 (Natural Image) 란 일반적으로 자연에서 관찰되는 장면이나 객체를 담은 이미지이다. 이는 풍경, 인물 사진 등 다양한 현실 세계의 모습을 표현한다.

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/JSM/figure1.jpg" height = 300>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 1 : Illustrations for Local Smoothness and Local Self-similarity of Natural Images ]</figcaption>
</figure>


#### - Properties
1. 비정형성과 복잡성
정형화된 패턴이 적고 예측하기 어려운 비정형적 요소가 많이 포함
2. 자연스러운 색 분포
특정 색상이 두드러지기보다 여러 색상이 자연스럽게 조화를 이룸
3. 통계적 자기 유사성
특정 패턴이나 질감이 다양한 공간적 스케일에서 유사하게 반복
4. 노이즈와 변동성
빛, 날씨, 카메라 센서의 민감도 등 다양한 외부 요인으로 인해 노이즈가 포함


## 1.2. Joint Statistical Modeling
본 논문에서는 기존 regularized inverse problem 의 정규화 텀을 바꿔 다음과 정의한다.

$$
\Psi_{JSM}(u) = \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM}(u)
$$

여기서 $\Psi_{LSM}$ 은 지역적 부드러움에 대한 사전분포, $\Psi_{NLSM}$ 은 비지역적 자기 유사성을 반영하는 사전분포이다. 또한, $\tau$ 와 $\lambda$ 는 각각  $\Psi_{LSM}$ 와 $\Psi_{NLSM}$ 을 조절하는 파라미터이다.



## 1.3. Local Statistical Modeling for Smoothness in Space Domain
지역적 부드러움(local smoothness) 는 2차원 공간 영역에서 인접한 픽셀의 유사성을 의미한다. 다음 figure2 의 (a) 는 고주파 대역 필터를 통과한 이미지를 나타내고, (b) 는 주파수 영역에서 주파수의 분포를 나타낸다.

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/JSM/figure2.jpg" height = 300>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 2 : Illustrations for LSM for Smoothness in Space Domain at Pixel Level ]</figcaption>
</figure>

#### - Vertical & Horizontal Filter

$$
\mathcal{D}_v = [1 -1]^T, \mathcal{D}_h = [1 -1], 
$$

수직 및 수평 필터는 이웃한 픽셀이 유사하지 않은 경우를 표현한다. 따라서 이 경우 픽셀의 분포는 figure2 (b) 와 같이 나타나며 대부분의 값들이 0 에 모여있는 것을 확인할 수 있다.

#### - Generalized Gaussian Distribution (GGD)
수직 및 수평 필터의 주변 확률 분포는 보통 GGD 로 표현되고, 이는 다음과 같이 정의된다.

$$
p_{GGD} = \frac{\mathbf{v} \cdot \eta(v)}{2 \cdot \Gamma(1/\mathbf{v})} \cdot \frac{1}{\sigma_\mathbf{x}} \cdot \exp \left[-({ \frac{\eta(\mathbf{v}) \cdot |\mathbf{x}|}{\sigma_\mathbf{x}}} ) ^\mathbf{v} \right]
$$

여기서 $\eta(\mathbf{v}) = \sqrt{\Gamma(3/\mathbf{v}){\Gamma(1/\mathbf{v})}}, \: \Gamma(t) = \int_0^\infin e^{-u}u^{t-1} du $ 는 감마 함수, $\sigma_x$ 는 표준편차, $\mathbf{v}$ 는 분포의 형태를 결정하는 parameter 이다. $\mathbf{v}$  값에 따른 분포는 다음과 같다.  

$$
\begin{split}
p_{GGD} (\mathbf{X}) = \begin{cases} Gaussian \: Dist'n & \text{if} \quad \mathbf{v} = 2 \\ 
Laplacian \: Dist'n & \text{if} \quad \mathbf{v} = 1 \\
hyper-Laplacian & \text{if} \quad 0 < \mathbf{v} < 1 \\ \end{cases} \\
\end{split}
$$

본 논문에서는 이미지의 통계량에 대한 정확한 묘사와 최적화 문제 간 균형을 맞추기 위해 $\mathbf{v} = 1$ 인 Laplacian 분포를 적용한다. 따라서 LSM 을 다음과 같이 정의한다.

$$
\Psi_{LSM}(u) = ||\mathcal{D}u||_1 = ||\mathcal{D}_v u||_1  + ||\mathcal{D}_h u||_1
$$

이는 anisotropic TV 와 유사한 형태이며 LSM 이 이미지 내 부드러움을 강조하는데만 사용된다는 것을 의미한다. 그리고 볼록 최적화와 계산 복잡성이 낮다는 장점을 갖는다. 


## 1.4. Nonlocal Statistical Modeling for Self-Similarity in Transform Domain
Nonlocal Statistical Modeling (NLSM) 은 LSM 과 다르게 자연이미지의 비국소적 부분의 반복적인 특징을 나타낸다. 그리고 이미지의 깔끔함과 엣지를 유지하는데 사용되기 때문에 비국소적 특징 보존에 효과적이다. 

#### - NLSM Process
NLSM 을 구성하는 과정은 다음 그림과 같다.

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/JSM/figure3.jpg" height = 130>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 3 : Illustrations for NLSM for Self-similarity in 3-D Transform Domain at Block Level ]</figcaption>
</figure>

step 1. 크기가 $N$ 인 이미지를 $n$ 개의 겹치는 블록 $u^i$ 로 나눈다. 여기서 각 블록 $u^i$ 는 크기가 $b_s$ 이다. 
step 2. 크기가 $L \times L$ 인 윈도우 내 블록 $u^i$ 중 가장 유사한 블록 $C$ 개를 탐색한다. 여기서 $C$ 를 고정된 숫자로 설정하며 유클리드 거리로 유사성을 측정한다. 유사한 블럭 집합 $S_{u^i}$ 을 다음과 같이 정의한다.

$$
S_{u^i} = \{ S_{u^i \otimes 1}, ... , S_{u^i \otimes c} \} 
$$  

step 3. 각 $S_{u^i}$ 에 대해, $S_{u^i}$ 에 속하는 $C$ 개의 블록을 3차원 배열로 쌓아 $Z_{u^i}$ 로 나타낸다.

step 4. 3차원 변환 연산자 $T^{3D}$ 에 대해, $T^{3D}(Z_{u^i})$ 를 $Z_{u^i}$ 의 변환 계수로 정의한다. 그리고 $\Theta_u$ 는 모든 변환 계수에 대한 열벡터이며, 크기가 $K = b_s*c*n $ 인 이미지 $u$ 에 맞추어 $T^{3D}(Z_{u^i})$ 를 사전식 순서로 정렬해 구성한다. 

step 5. Figure3 의 가장 오른쪽 그림처럼 변환 계수의 히스토그램을 분석한다. 0을 중심으로 매우 날카로운 것을 확인할 수 있으며, LSM과 유사하게 GGD로 특징을 표현할 수 있다. 따라서 LSM 과 유사한 이유로 $\Theta_u$ 의 분포를 Laplacian 함수로 모델링한다. 그리고 NLSM 을 다음과 같이 정의한다.

$$
\Psi_{NLSM} (u) = ||\Theta_u||_1 = \sum_{i=1}^n ||T^{3D}(Z_{u^i})||_1
$$

#### - Inverse Operator $\Omega_{NLSM}$
1. $\Theta_u$ 를 계산 후, $n$ 개의 3차원 변환계수로 구성된 3차원 배열로 분할
2. 이를 역변환해 3차원 배열의 각 블록에 대한 추정치 생성
3. 각 블록 단위로 추정된 값은 원래 위치로 반환되고, 모든 블록 단위 추정치에 대한 평균을 계산해 최종 이미지의 추정치를 계산. 


## 1.5. Summary
따라서 JSM은 다음과 같이 정의할 수 있다.

$$
\Psi_{JSM}(u) = \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM}(u) = \tau \cdot || \mathcal{D} u ||_1 + \lambda \cdot || \Theta_u ||_1 
$$





# 2. Split Bregman based Iterative Algorithm for Image Restoration using JSM
## 2.1. Regularized Inverse Problem 
이미지 복원을 위해 새롭게 정의한 정규화 역문제는 다음과 같다.

$$
\argmin_u \frac{1}{2} ||Hy - u||_2^2 + \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM} (u)
$$


## 2.2. Split Bregman Iteration
Split Bregman Iteration (SBI) 는 최소화 문제에서 $l_1$ norm 을 해결하기 위해 제안되었다. SBI 의 아이디어는 제약이 없는 최소화 문제에 제약을 추가해 변수를 분리함과 동시에 Bregman 반복을 진행하는 것이다. 

#### - SBI Algorithm
$G \in \mathbb{R}^{M \times N}, \: f : \mathbb{R}^N \rightarrow \mathbb{R}, \:  g : \mathbb{R}^M \rightarrow \mathbb{R} $ (적절한 닫힌 볼록함수) 에 대해 제약이 없는 최적화 문제 $\min_{u \in \mathbb{R}^N} f(u) + g(Gu)$ 를 해결하기 위한 SBI 는 다음과 같다.

1. Set $k = d^{(0)} = u^{(0)} = v^{(0)} = 0$ and choose $\mu > 0$
2. Repeat

$$
\begin{split}
&u^{(k+1)} = \argmin_u f(u) + \frac{\mu}{2} ||Gu-v^{(k)}-d^{(k)}||_2^2 \\ 
&v^{(k+1)} = \argmin_v g(v) + \frac{\mu}{2} ||Gu^{(k+1)}-v-d^{(k)}||_2^2 \\ 
&d^{(k+1)} = d^{(k)} - (Gu^{(k+1)}-v^{(k)}) \\ 
&k = k+1
\end{split}
$$

3. Until stopping criterion is satisfied

#### - Applying SBI Algorithm 
$f(u) = \frac{1}{2} ||Hu-y||_2^2, \: g(v) = g(Gu) = \Psi_{JSM}(u), d^{(k)} = \begin{bmatrix} b^{(k)} \\ c^{(k)} \end{bmatrix} \in \mathbb{R}^{2N}, b^{(k)} \& \: c^{(k)}\in \mathbb{R}^N  $ 에 대해 최적화 문제 $ \argmin_{u \in \mathbb{R}^v, v \in \mathbb{R}^{2N}} f(u) + g(v) \:\: s.t. \:\: Gu = v $ 를 해결하기 위한 SBI 적용하면 각 변수를 다음과 같이 표현할 수 있다. 

1. Calculating $u$

$$
\begin{split}
u^{(k+1)} &= \argmin_u f(u) + \frac{\mu}{2} ||Gu-v^{(k)}-d^{(k)}||_2^2 \\
&= \argmin_u \frac{1}{2} \| Hu - y \|_2^2 + \frac{\mu}{2} \left\| \begin{bmatrix} I \\ I \end{bmatrix} u - \begin{bmatrix} w^{(k)} \\ x^{(k)} \end{bmatrix} - \begin{bmatrix} b^{(k)} \\ c^{(k)} \end{bmatrix} \right\|_2^2 \\
&= \argmin_u + \frac{\mu}{2} \| u + w^{(k)} + b^{(k)} \|_2^2 + \| u + x^{(k)} + c^{(k)} \|_2^2 
\end{split}
$$



2. Calculating $v$
$v$ 에 대한 식을 정리하면 $w$ 와 $x$ 로 각각 분리가 가능하다.

$$
\begin{split}
v^{(k+1)} &= \begin{bmatrix} w^{(k+1)} \\ x^{(k+1)} \end{bmatrix} \\
&= \argmin_{w,x} \tau \cdot \Psi_{LSM}(w) + \lambda \cdot \Psi_{NLSM} (x) + \frac{\mu}{2} \left\| \begin{bmatrix} I \\ I \end{bmatrix} u^{(k+1)} - \begin{bmatrix} w \\ x \end{bmatrix} - \begin{bmatrix} b^{(k)} \\ c^{(k)} \end{bmatrix} \right\|_2^2 \\
&= \argmin_{w,x} \tau \cdot \Psi_{LSM}(w) + \lambda \cdot \Psi_{NLSM} (x) + \frac{\mu}{2} ||u^{(k+1)}-w-b^{(k)}||_2^2 + \frac{\mu}{2} ||u^{(k+1)}-x-c^{(k)}||_2^2 \\
& \rightarrow \begin{cases} w^{(k+1)} &= \argmin_w \tau \cdot \Psi_{LSM} (w) + \frac{\mu}{2} \| u^{(k+1)} - w - b^{(k)} \|_2^2 \\ 
x^{(k+1)} &= \argmin_x \lambda \cdot \Psi_{NLSM} (x) + \frac{\mu}{2} \| u^{(k+1)} - x - c^{(k)} \|_2^2 \\ \end{cases}
\end{split}
$$


3. Calculating $d$
$d$ 에 대한 식을 정리하면 $b$ 와 $c$ 로 각각 분리가 가능하다.

$$
\begin{split}
d^{(k+1)} &= \begin{bmatrix} b^{(k+1)} \\ c^{(k+1)} \end{bmatrix} - \left( \begin{bmatrix} I \\ I \end{bmatrix} u^{(k+1)} - \begin{bmatrix} w^{(k+1)} \\ x^{(k+1)} \end{bmatrix} \right) \\
& \rightarrow \begin{cases} b^{(k+1)} &= b^{(k)} - (u^{(k+1)} - w^{(k+1)}) \\ 
c^{(k+1)} &= c^{(k)} - (u^{(k+1)} - x^{(k+1)}) \\ \end{cases}
\end{split}
$$


4. Theorem 1
SBI 가 더 빠르게 수렴하기 위해 다음과 같은 정리를 이용한다.

$$
\begin{split}
The \: proposed \: algo&rithm \: described \: by \: Table \: I \: converges \: to \: a \: solution \: of \: \\
\argmin_u &\frac{1}{2} ||Hy - u||_2^2 + \tau \cdot \Psi_{LSM}(u) + \lambda \cdot \Psi_{NLSM} (u)
\end{split}
$$

proof) 
제안한 알고리즘이 SBI 의 한 예시라는 것은 자명하다. $f(\cdot), \Psi_{LSM}(\cdot), \Psi_{NLSM}(\cdot) $ 이 닫히고 적절하고 볼록한 형태이기 때문에, 제안한 알고리즘의 수렴은 다음과 같은 full column rank 행렬 $G$ 로 보장된다.

$$
G = \begin{bmatrix} I \\ I \end{bmatrix} \in \mathbb{R}^{2N \times N} \qquad \Box
$$ 


5. Summary
따라서 정리한 식들에 대해 알고리즘을 정리하면 다음과 같다.

Input : 관측한 이미지 $y$ & 선형 행렬 연산자 $H$
Initialization : $k = b^{(0)} = c^{(0)} = w^{(0)} = x^{(0)} = 0$  and  choose $\mu, \tau, \lambda$

Repeat 

$$
\begin{split}
&u = \argmin_u \frac{1}{2} \| Hu - y \|_2^2 + \frac{\mu}{2} \| u - w^{(k)} - b^{(k)} \|_2^2 + \| u - x^{(k)} - c^{(k)} \|_2^2 \\
&p^{(k)} = u^{(k+1)} - b^{(k)}; \quad \nu = \tau/\mu \\
&w^{(k+1)} = prox_{\nu} (\Psi_{LSM})(p^{(k)})  \\
&r^{(k)} = u^{(k+1)} - c^{(k)}; \quad \alpha = \lambda/\mu \\
&x^{(k+1)} = prox_{\alpha} (\Psi_{NLSM})(r^{(k)}) \\
&b^{(k+1)} = b^{(k)} - (u^{(k+1)} - w^{(k+1)}) \\
&c^{(k+1)} = c^{(k)} - (u^{(k+1)} - x^{(k+1)}) \\
\end{split}
$$

Until stopping criterion is satisfied
Output : Final restored image $u$


## 2.3. Choosing Variables
#### - Choosing $u$

$$
u = \argmin_u \frac{1}{2} \| Hu - y \|_2^2  + \frac{\mu_1}{2} \| u - w - b \|_2^2 + \frac{\mu_2}{2} \| u + x + c \|_2^2 
$$

이를 $u$ 에 대해 편미분 후 정리하면 다음과 같이 표현가능하다.

$$
u = (HH^T + \tilde{\mu} I)^{-1} \cdot q \quad where \quad  q = H^T y + \mu_1 (w+b) + \mu_2 (x+c) \quad  and \quad \tilde{\mu} = \mu_1 + \mu_2
$$

1. Image Inpainting
선형 연산자 $H$ 를 이진 대각행렬로 정의한다. 따라서 $u$ 를 다음과 같이 표현할 수 있다.

$$
\begin{split}
H = &diag(1, ... , 1, 0, 1, ... , 1) \quad and \quad HH^T = I \\
\rightarrow \quad &u = \frac{1}{\tilde{\mu}} ( I - \frac{1}{1+\tilde{\mu}} H^T H) \cdot q
\end{split}
$$

여기서 $H$ 의 대각행렬 중 1 은 픽셀이 존재하는 위치, 0은 픽셀이 존재하지 않는 위치를 나타낸다. 또한, 위 식은 역행렬연산이 필요없기 때문에 매우 효율적으로 계산이 가능하다.


2. Image Deblurring
선형 연산자 $H$ 를 순환 합성곱 행렬(deblurring filter)로 정의한다. 따라서 $u$ 를 다음과 같이 표현할 수 있다.

$$
\begin{split}
H = &U^{-1}DU  \\
\rightarrow \quad &u = (U^{-1}D^*DU + \tilde{\mu}U^{-1}U)^{-1} = U^{-1}(|D|^2 + \tilde{\mu}I)^{-1}U 
\end{split}
$$

여기서 U 는 2차원 DFT 행렬, D 는 H로 표현되는 합성곱 연산자의 DFT 계수를 나타내는 대각행렬, $(\cdot)^*$ 은 켤례 복소수를 의미한다. 실제로 $U^{-1}U$ 연산이 FFT 알고리즘에 의해 연산비용이 $O(N \log N)$ 으로 효율적인 연산이 가능하다.



#### - Choosing $w$
$\| Du \|_1 $ 의 non-smoothness 를 해결하기 위해 Fast Iterative Shrinkage-Thresholidng Algorithm (FISTA) 를 사용한다.

1. FISTA Algorithm
$f(x) = \frac{1}{2} \| Hu - y \|_2^2, g(x) $ : 정규화 항인 최적화 문제 $\argmin_x (F(x) = f(x) + g(x))$ 에 대해 다음 순서로 알고리즘을 적용한다.

$$
\begin{split}
&1.\mathbf{Initilaize} \quad \: initial \: value \: x_0, \quad auxiliary \: variable \: y_1 = x_0 \quad acceleration \: parameter \: t_1 = 1   \\
&2. \mathbf{Calculate} \quad x_{k+1} = prox_{\alpha, g}(y_k - \alpha \nabla f(y_k)) \quad \alpha : step \: size \: or \: learning \: rate \\
&3. \mathbf{Update} \qquad t_{k+1} = \frac{1+\sqrt{1+4t_k^2}}{2} \quad and \quad y_{k+1} = x_{k+1} + \frac{t_k - 1}{t_{k+1}} (x_{k+1} - x_k) \\
& \mathbf{Until} \:\: Convergence \\
\end{split}
$$


#### - Choosing $x$
$x$ 에 노이즈가 추가된 관측치인 $r$ 에 대해 $x$ 를 다음과 같이 표현할 수 있다.

$$
\begin{split}
x &= prox_{\alpha} (\Psi_{NLSM}) (r) \\
&= \argmin_x \frac{1}{2} \| x-r \|_2^2 + \alpha \cdot \Psi_{NLSM}(x) \\
&= \argmin_x \frac{1}{2} \| x-r \|_2^2 + \alpha \cdot \| \Theta_x \|_1
\end{split}
$$

1. Residual Distribution
이미지의 잔차에 대한 분포를 확인하기 위해 $e = x-r$ 의 통계량을 관찰한다. 예시 이미지로 $Butterfly$ 컬러 이미지를 사용하였는데, 원본 이미지에 Gaussian blur kernel을 적용하였고 표준편차가 0.5 인 Gaussian white noise 를 추가하였다. $r^{(k)} = u^{(k)} - c^{(k-1)} $ 에 대해 $e^{(k)} = x^{(k)} - r^{(k)} $ 과 분산 $ Var(e^{(k)}) = \frac{1}{N} \| x^{(k)} - r^{(k)} \|_2^2 $ 의 히스토그램을 관찰하였다. 

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/JSM/figure6.jpg" height = 250>    
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure 4 : Distribution of Residuals and its Varaiance ]</figcaption>
</figure>

Figure 4 를 확인하면, 각 반복에서 잔차가 평균이 0이고 분산이 $Var(e^{(k)})$ 인 GGD 를 따르는 것과 유사한 것을 알 수 있다. 또한 각 반복의 잔차들이 서로 연관성이 없으며 독립이라는 것도 알 수 있다. 따라서 다음과 같은 정리를 통해 분포에 대한 가정을 하고자 한다. 

2. Theorem 2
$x \& r \in \mathbb{R}^N, \: \Theta_x \& \Theta_r \in \mathbb{R}^K$ 에 대해 잔차 $e = x-r$ 와 $j=1,...,N$ 에 대해 잔차 $e$ 의 각 원소를 $e(j)$ 로 정의한다. 이 때 $e(j)$ 는 서로 독립이고 평균이 0이고 분산이 $\sigma^2$ 인 분포를 따른다. 그러면 모든 $\epsilon > 0 $ 에 대해 다음이 성립한다.

$$
\lim_{N->\infin, K->\infin} \mathbb{P} \left [\left | \frac{1}{N} \| x-r \|_2^2 - \frac{1}{N} \|\Theta_x - \Theta_r\|_2^2 < \epsilon \right| \right] = 1
$$

proof)
$e(j)$ 가 독립이라는 가정으로 $e(j)^2$ 도 독립이다. 또한 이는 평균이 0이고 분산이 $\sigma^2$ 이다. 대수의 법칙에 의해 다음이 성립한다.

$$
\begin{split}
&\lim_{N->\infin} \mathbb{P} \left [\left | \frac{1}{N} \sum_{j=1}^N e(j)^2 - \sigma^2 \ \right| < \frac{\epsilon}{2} \right] = 1, \\
&\lim_{N->\infin} \mathbb{P} \left [\left | \frac{1}{N} \| x-r \|_2^2 - \sigma^2 \ \right| < \frac{\epsilon}{2} \right] = 1 \\
\end{split}
$$

이 때 $\Theta_e$ 의 정의에 의해, $\Theta_e(j)$ 는 서로 독립이고 평균이 0에 분산이 $\sigma^2$ 이다. 따라서 동일한 방법으로 $\Theta_e(j)^2$ 에 대해 다음이 성립한다.

$$
\begin{split}
&\lim_{K->\infin} \mathbb{P} \left [\left | \frac{1}{K} \sum_{j=1}^K \Theta_e(j)^2 - \sigma^2 \ \right| < \frac{\epsilon}{2} \right] = 1, \\
&\lim_{K->\infin} \mathbb{P} \left [\left | \frac{1}{K} \| \Theta_x - \Theta_r \|_2^2 - \sigma^2 \ \right| < \frac{\epsilon}{2} \right] = 1  \quad \Box \\
\end{split}
$$

3. Conclusion

Theorem 2 에 의해 다음이 성립한다.

$$
\frac{1}{N} \| x^{(k)} - r^{(k)} \|_2^2 = \frac{1}{K} \| \Theta_x^{(k)} - \Theta_r^{(k)} \|_2^2 
$$

이를 $x$ 에 대입하면 다음과 같은 식을 얻을 수 있다.

$$
\argmin_x \frac{1}{2} \| \Theta_x - \Theta_r \|_2^2 + \frac{K\alpha}{N} \| \Theta_x \|_1
$$

이 때 미지의 변수 $\Theta_x$ 가 요소별로 분할이 가능하기 때문에 $\Theta_x(j)$ 은 다음과 같은 soft thresholding 을 이용해 독립적으로 계산할 수 있다.

$$
\begin{split}
&\Theta_x = soft(\Theta_r, \sqrt{2\rho}) \quad where \quad j = 1, ... ,K , \: \rho = \frac{K\alpha}{N} \: and \\
&\Theta_x (j) = sign(\Theta_r(j)) \cdot \max \left[ |\Theta_r(j)| - \sqrt(2\rho) \right] \\
&= \begin{cases} \Theta_r(j) - \sqrt{2\rho} \quad \text{if} \quad \Theta_r(j) \in \mathbb{R} (\sqrt{2\rho}, \infin) \\ \qquad \quad 0 \qquad \quad \text{if} \quad \Theta_r(j) \in \mathbb{R} (-\sqrt{2\rho}, -\sqrt{2\rho}) \\ \Theta_r(j) + \sqrt{2\rho} \quad \text{if} \quad  \Theta_r(j) \in \mathbb{R} (-\infin ,-\sqrt{2\rho}) \end{cases}
\end{split}
$$

최종적으로 $x$ 를 다음과 같이 표현한다.

$$
x = \Omega_{NLSM} (\Theta_x) = \Omega_{NLSM} (soft(\Theta_r, \sqrt{2\rho}))
$$



## 2.4. Summary of Proposed Algorithm

**Input** : 관측된 이미지 $y$ 와 선형 행렬 연산자 $H$
**Initialization** : $k = b^{0} = c^{0} = w^{0} = x^{0} = 0, u^{0} = y$
**Choose** : $\tau, \lambda, \mu_1, \mu_2$
**Repeat**

$$
\begin{split}
&Compute \quad u^{(k+1)} = \begin{cases} \frac{1}{\tilde{\mu}} \left( I - \frac{1}{1+\tilde{\mu}}H^TH \cdot q \right) \: \text{if} \: Inpainting \\ U^{-1} \left( |D|^2 + \tilde{\mu}I \right)^{-1} U \: \: \text{if} \: Deblurring \end{cases} where \:\: q = H^T y + \mu_1 (w+b) + \mu_2(w+c)\\
&p^{(k)} = u^{(k+1)} - b^{(k)}; \quad \nu = \tau / \mu_1 \\
&Compute \quad w^{(k+1)} \: by \:\: FISTA \\
&r^{(k+1)} = u^{(k+1)} - c^{(k)}; \quad \alpha = \lambda / \mu_2 \\
&Compute \quad x^{(k+1)} = \Omega_{NLSM}(soft(\Theta_r, \sqrt{2\rho})) \\
&b^{(k+1)} = b^{(k)} - (u^{(k+1)} - w^{(k+1)}) \\
&c^{(k+1)} = c^{(k)} - (u^{(k+1)} - x^{(k+1)}) \\
\end{split}
$$

**Until** 지정한 반복횟수
**Output** : 최종 복원한 이미지 $u$