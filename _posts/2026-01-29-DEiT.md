---
layout: single        # 문서 형식
title: Training Data-Efficient Image Transformers & Distillation Through Attention       # 제목
categories: NLP   # 카테고리
tag: [DL, NLP, Attention, Transformer, Knowledge Distillation]
sidebar_main : true # 좌측 글 목록

author_profile: true  # 사이드바 visible 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "counts"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
Attention, Transformer, Knowledge Distillation, NLP


# 1. Introduction



# 2. VQ-VAE
## 2.1. VAE
#### - Notation
$$ \mathcal{E} $$ : encoder

$$ \mathcal{D} $$ : decoder

$$ x $$ : input data

$$ z $$ : discrete latent random variable

$$ p(z) $$ : a prior distribution

$$ q(z \vert x) $$ : posterior distribution

$$ p(x \vert z) $$ : a distribution for input $$x$$ given $$z$$


#### - Architecture

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/vqvae1.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 1 : The type of directed graphical model of VAE under consideration ]</b> 
</p>

**1. 인코더**

입력 데이터 $$x$$ 를 잠재 공간의 평균($$\mu$$) 와 표준편차 ($$\sigma$$) 값으로 변환한다.


**2. 잠재 공간**
데이터의 핵심 특징이 압축된 공간이다. VAE 내 사전 확률 분포와 사후 확률 분포는 모두 가우시안 분포를 가정한다 ($$\sum \mathcal{N}(0, \Sigma) \text{ where } \Sigma \; : \; diagonal matrix $$). 이를 적용함으로서 재모수화를 사용해 샘플링을 진행하는데, 이는 모델이 미분 가능하여 학습될 수 있도록 돕는다.

**3. 디코더**
잠재 공간에서 샘플링된 벡터를 다시 원래 데이터 형태로 복원한다.


#### - Variational
VAE의 목표는 단순히 똑같은 이미지를 만드는 것이 아니라, 그럴듯한 새로운 데이터를 생성하는 것이다. 

**1.연속적인 잠재 공간**

데이터를 분포로 학습하기 때문에, 잠재 공간의 특정 지점들을 이동하며 데이터의 특징이 부드럽게 변하는 결과를 얻을 수 있다.


**2. 생성 능력**
학습이 끝난 후 디코더에 임의의 잠재 벡터를 넣어주면, 새로운 이미지를 생성한다. 

#### - 손실 함수
$$Loss = \mathcal{L}_{Recon} + \text{KL Divergence}$$

$$\mathcal{L}_{Recon}$$ : 복원 손실. 출력값이 입력값과 얼마나 비슷한지를 측정

$$\text{KL Divergence}$$ : 학습된 잠재 공간의 분포가 사전에 가정한 가우시안 분포와 너무 멀어지지 않도록 규제.


## 2.2. Discrete Latent Variables
#### - Notation

$$K$$ : 이산 잠재 공간의 크기

$$D$$ : 각 임베딩 벡터 $$e_i$$ 의 차원(특징)

$$e_i$$ : 사전에 정의된 임베딩 벡터

$$e \in \mathbb{R}^{K \times D}$$ : 잠재 임베딩 공간



#### - 인코딩
입력값 $$x$$ 가 인코더 $$\mathcal{E}$$ 를 통과해 결과물 $$z_e(x)$$ 를 생성한다. 즉, 잠재 공간으로의 임베딩 벡터로 변환한다. 그리고 이산 잠재 변수인 $$z$$ 는 다음과 같은 식을 통해 사전에 정의된 임베딩 벡터 $$e$$ 로 근사된다.

$$
\begin{split}
  q(z=k \vert x) = 
  \begin{cases} 
    &1 \text{ for } k = argmin_j \Vert z_e(x) - e_i \Vert_{2} \\
    &0 \text{ otherwise }
  \end{cases}
\end{split}
$$

여기서 $$z_e(x)$$ 는 인코더를 통과한 후의 벡터이다. 이는 원-핫 인코딩을 이용한 사후 범주 확률 분포이다. 이후 디코더의 입력값은 다음과 같다.

$$
\begin{split}
  z_q(x) = e_k, \quad \text{where} \quad k=argmin_j \Vert z_e(x) - e_j \Vert_{2}
\end{split}
$$

모수가 존재하는 부분은 인코더, 디코더, 임베딩 공간까지 총 3개이다. 위와 같은 식들을 이용해 임베딩 벡터로 만드는 이유는 우리가 음성이나 이미지, 비디오와 같은 데이터에서 각각 1, 2, 3차원 데이터를 뽑고 이를 이산적으로 표현할 수 있기 때문이다. 필자들은 VAE를 ELBO로 $$\log p(x)$$ 의 경계를 설정할 수 있다는 부분에 초점을 맞춘다. 제안한 사후 범주 확률 분포는 결정적(deterministic)이고 $$z$$ 에 대한 간단한 균등 사전 분포를 정의했기 때문에, KL 발산 상수를 $$\log K$$ 로 이용할 수 있다.



## 2.3. Learning
<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure1.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 2 : A Figure Describing the VQ-VQE ]</b> 
</p>

$$z_q(x)$$ 의 경우 실제 기울기(gradient) 가 없기 때문에, Straight-Through Estimator (STE) 를 이용한다. 즉, 디코더 입력값인 $$z_q(x)$$ 부터 인코더의 결과값인 $$z_e(x)$$ 의 값을 복사해 사용한다. 이 기법을 이용해 인코더는 특징 추출 능력을, 디코더는 생성 능력을 동시에 학습할 수 있게 된다. 학습 중에는 인코더와 디코더가 동일한 크기의 차원을 공유하기 때문에 디코더가 가리키는 방향을 인코더가 그대로 따라가기만 해도 학습이 성공적으로 이루어진다. 

#### - Loss Function
전체 손실함수는 다음과 같이 세 가지 항으로 구성된다.

$$
\begin{split}
  L = \log p(x \vert z_q(x)) + \Vert sg[z_e(x)] - e \Vert_{2}^{2} + \beta \cdot \Vert z_e(x) - sg[e] \Vert_{2}^{2}
\end{split}
$$

첫 번째 항은 STE를 이용해 디코더와 인코더를 최적화하는 복원 손실이다. $$z_e(x)$$ 부터 $$z_q(x)$$ 로 매핑된 STE 로 인해 임베딩 $$e_i$$ 는 복원 손실 $$\log p(z \vert z_q(x))$$ 로부터 어떠한 기울기를 받지 않는다. 두 번째 항은 코드북 벡터 $$e$$ 가 인코더 출력값 $$z_e(x)$$ 과 가까워지도록 업데이트함으로써 코드북 학습을 유도한다. 마지막 항은 commitment 손실로 인코더 출력값이 코드북 벡터에서 너무 멀어지지 않도록 강제한다. 잠재 공간의 부피는 제한이 없기 때문에 출력값의 변동을 방지해 인코더가 특정 코드북 벡터 범위 내에 존재하도록 유도한다. VAE와 달리 사전 분포를 균등 분포로 가정했기에 앞서 말한데로 인코더 파라미터에 대해 KL 발산항이 상수로 되어 최적화 과정에서 무시할 수 있다. 전체 모델 $$\log p(x)$$ 의 로그 가능도는 다음과 같이 표현할 수 있다.

$$
\begin{split}
  \log p(x) = \log \sum_k p(x \vert z_k) \; p(z_k),
\end{split}
$$

MAP 추론 방식으로부터 $$z=z_q(x)$$ 라는 사실을 이용해 디코더 $$p(x \vert z)$$ 는 학습되었기 때문에, 모델이 수렴하면 디코더는 선택되지 않은 다른 벡터 $$(z \neq z_q(x))$$ 에 대한 확률은 배제한다. 따라서 모델을 다음과 같이 근사할 수 있다.

$$
\begin{split}
  \log p(x) \approx \log p(x \vert z_q(x)) \cdot p(z_q(x))
\end{split}
$$

따라서 Jensen 부등식을 이용하면, 다음과 같이 표현할 수 있다.

$$
\begin{split}
  \log p(x) \le \log p(x \vert z_q(x)) \cdot p(z_q(x))
\end{split}
$$


## 2.4. Prior
모델을 학습할 때는 사전 분포 $$p(z)$$ 를 균등 분포로 고정한다. 왜냐하면 인코더와 디코더가 먼저 데이터의 핵심 특징을 뽑아내고 코드북을 구성하는 데에만 집중하도록 유도하기 위함이다. 이후 학습이 끝나면 이미지는 코드북 인덱스의 나열로 치환할 수 있다. 저자들은 PixelCNN을 이용해 이미지 데이터를 처리하고 음성 데이터에 대해서는 WaveNet을 이용했다. 




# 3. Experiments
## 3.1. Comparison with continuous variables
#### - Experimental Setup
공정한 비교를 위해 동일한 구조의 VAE 아키텍처를 사용해 VAE, VQ-VAE, VIMCO(이산적 최적화 방식)를 비교한다. 데이터셋은 CIFAR-10을 이용하며 인코더의 경우 2개의 strided convolution 층, 2개의 residual 블록, 모든 은닉층은 256개로 구성한다. 디코더의 경우 인코더의 역순으로 구성한다(Transposed Convolution). ADAM (학습률 : 2e-4, batch 크기 : 128)을 이용해 최적화하며 총 25만번(steps) 학습을 반복한다.

#### - Comparison
성능은 bits/dim (차원당 비트 수)으로 측정했으며, 이 수치가 낮을수록 모델이 데이터를 더 잘 압축하고 표현했다는 것을 의미한다. VAE, VQ-VAE, VIMCO 가 순서대로 4.51, 4.67, 5.14 를 기록했다. 특히, VQ-VAE 는 이산적인 잠재 변수를 사용했음에도 불구하고, 전통적인 연속형 VAE 의 성능에 거의 근접하는 결과를 보인다. 이는 이산적 표현이 가진 압축 효율성과 연속형 모델이 가진 유연함을 동시에 잡았음을 의미한다.


## 3.2. Images
VQ-VAE 가 고해상도 이미지를 어떻게 이산적으로 압축하고, 학습된 모델이 새로운 이미지를 얼마나 잘 생성하는지 보여준다.

#### - Compression and Reconstruction

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure2.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 3 : ImageNet Samples 1 ]</b> 
</p>

$$128 \times 128 \times 3$$ 크기의 이미지를 잠재 공간으로 압축한 뒤 다시 복원한다. 이 때 이미지를 $$32 \times 32 \times 1$$ 크기의 이산적인 인덱스 맵으로 압축한다. 원본 데이터보다 약 42.6배나 작은 크기로 정보를 압축했음에도 불구하고, 복원된 이미지는 시각적으로 매우 선명해 원본의 핵심 특징을 그대로 유지한다. 


<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure3.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 4 : Samples from a VQ-VAE with a PixelCNN Prior Trained on ImageNet Images ]</b> 
</p>

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure4.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 5 : Samples from a VQ-VAE with a PixelCNN Prior Trained on Frames Captured from DeepMind Lab ]</b> 
</p>

#### - Generation by Prior Training
인코더 및 디코더 학습이 끝난 후, 잠재 변수들의 분포를 학습하기 위해 PixelCNN을 사용한다. 학습된 VQ-VAE 위에서 PixelCNN 이 이산적인 인덱스(latent codes)를 생성하고 하고, 이를 디코더에 통과시켜 새로운 이미지를 생성한다. 기존 VAE가 평균적인 이미지를 만드려다 뿌연 결과물을 내놓는 것과 달리, VQ-VAE는 텍스쳐가 살아있고 구조적으로 완성도가 높은 고화질 이미지를 생성한다. 따라서, 이산적인 코드북을 사용함으로써 확률 모델의 모호함을 제거하고 선명한 경계선을 가진 이미지를 생성하게 될 수 있음을 의미한다. 또한 픽셀 하나하나를 예측하는 대신, 압축된 핵심 의미 단위를 예측함으로써 훨씬 효율적인 이미지 생성이 가능해졌다는 점을 시사한다.

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure5.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 6 : Image Generation Results Comparison ]</b> 
</p>



## 3.3. Audio
이미지 실험이 선명함을 증명했다면, 음성 실험은 VQ-VAE 가 데이터의 본질적인 내용과 부차적인 정보를 얼마나 잘 분리했는지 보여준다.

#### - Experimental Setup
저자들은 아무런 전처리 없는 Raw Audio 데이터를 입력으로 사용한다. 데이터셋은 다양한 화자의 음성을 포함하는 VCTK 을 사용하고, 압축률의 경우 원본 오디오를 64배 압축했다. 디코더는 압축된 신호를 다시 소리로 복원하기 위해 강력한 음성 생성 모델인 WaveNet 아키텍쳐를 채택했다.


#### - Results
VQ-VAE 는 학습 과정에서 음성 파형으로부터 무슨 말을 하는지에 대한 이산적 코드를 추출한다. 동시에 누가 말하는지에 대한 정보는 화자 임베딩으로 따로 분리된다. 이를 이용해 A 라는 사람이 말한 내용을 B 라는 사람의 목소리로 재생하는 데이터를 얻을 수 있다. 이는 비지도 학습임에도 불구하고 복원된 음성의 품질이 매우 뛰어났으며, 모델이 음성에서 음소(phoneme)와 유사한 고차원적 특징을 스스로 학습했음을 의미한다. 즉, VQ-VAE는 언어적인 라벨없이도 스스로 소리에서 의미있는 단위를 찾아내며, 매우 적은 양의 이산적 데이터만으로도 원본의 뉘앙스를 완벽하게 복원가능함을 입증했다.

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure6.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 7 : Audio Reconstruction Results Comparison ]</b> 
</p>




## 3.4. Video
비디오 데이터는 프레임 간 연속성이 중요하기 때문에 학습이 매우 까다롭다. VQ-VAE 는 비디오를 이산적인 코드로 압축해 이 어려움을 극복한다.

#### - Experimental Setup
데이터셋은 DeepMind Lab 환경에서 수집된 비디오 시퀀스를 사용하는데, 이는 에이전트가 미로를 돌아다니는 1인칭 시점 비디오다. 과거의 프레임들을 보고, 앞으로 발생할 미래의 프레임을 예측해 생성하는 것을 목표로 진행한다. 이 떄 각 비디오 프레임을 VQ-VAE 로 압축해 이산적인 코드 시퀀스로 변환한다. 이후 코드들의 시공간적 패턴을 학습하기 위해 사전확률분포 모델을 사용한다.

#### - Results

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/figure7.png" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 8 : Video Generation Results Comparison ]</b> 
</p>

픽셀 단위로 직접 미래를 예측하는 모델들은 시간이 갈수록 결과가 뭉개지거나 논리적이지 않은 영상들을 만든다. 하지만 VQ-VAE 는 압축된 핵심 코드만을 예측하기 때문에 훨씬 더 긴 시간 동안 일관성 있는 미래 영상을 생성할 수 있었다. 또한 비디오 전체를 생성하는 대신 잠재 공간 상에서 연산이 이루어지기 때문에 계산 비용이 대폭 감소했다. 이를 통해 모델이 단순히 이미지를 그리는 법을 넘어, 환경이 어떻게 변화할지를 이산적인 기호로 이해하고 있음을 확인할 수 있다. 또한, 비디오처럼 정보량이 방대한 데이터일수록 VQ-VAE 특유의 강력한 압축 성능이 빛을 발한다는 점을 시사한다.



# 4. Conclusion
VQ-VAE 는 잠재 변수를 이산화함으로써 정보의 병목을 효율적으로 생성했다. 그 결과, 원본 데이터를 수십 배 압축하면서도 VAE 특유의 뭉게짐(Blurry) 현상 없이 매우 선명한 복원 성능을 증명했다. 실험에서도 확인 가능하듯이, 이는 특정 데이터에 국한되지않는다. 그리고 데이터를 잘 압축하고, 그 압축된 코드의 규칙을 나중에 학습한다는 2단계 학습 전략을 표준으로 만들었다. 