---
layout: single        # 문서 형식
title: Neural Discrete Representation Learning(2018)        # 제목
categories: Generative Model   # 카테고리
tag: [DL, Image, AE, Quantization]
sidebar_main : true # 좌측 글 목록

author_profile: true  # 사이드바 visible 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "counts"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
Image Generation, AutoEncoder, Variational AutoEncoder, Vector Quantization, Discrete


# 1. Introduction
기존의 VAE 는 잠재공간(latent space)을 연속적인 확률 분포(가우시안 확률 분포)로 가정한다. 하지만 우리가 다루는 데이터(이미지, 음성, 텍스트)는 이산적인 속성을 갖고 있다. 기존의 모델들은 데이터의 본질적 특징들을 이해하지 못하며 사후 확률 붕괴 현상이 존재한다. 본 논문에서는 벡터 양자화 (vector quatization, VQ)를 도입해 이와 같은 문제들을 해결하고자 하며 더 효율적이고 뛰어난 성능을 실험적으로 증명한다. 


# 2. VQ-VAE
## 2.1. VAE
#### - Notation
$$ \mathcal{E} $$ : encoder

$$ \mathcal{D} $$ : decoder

$$ x $$ : input data

$$ z $$ : discrete latent random variable

$$ p(z) $$ : a prior distribution

$$ q(z \vert x) $$ : posterior distribution

$$ p(x \vert z) $$ : a distribution for input $$x$$ given $$z$$


#### - Architecture

<p align="center">
  <a href="#">
    <img src="/images/VQVAE/vqvae1.jpg" height="225" />
  </a>
  <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>[ Figure 1 : The type of directed graphical model of VAE under consideration ]</b> 
</p>

**1. 인코더**

입력 데이터 $$x$$ 를 잠재 공간의 평균($$\mu$$) 와 표준편차 ($$\sigma$$) 값으로 변환한다.


**2. 잠재 공간**
데이터의 핵심 특징이 압축된 공간이다. VAE 내 사전 확률 분포와 사후 확률 분포는 모두 가우시안 분포를 가정한다 ($$\sum \mathcal{N}(0, \Sigma) \text{ where } \Sigma \; : \; diagonal matrix $$). 이를 적용함으로서 재모수화를 사용해 샘플링을 진행하는데, 이는 모델이 미분 가능하여 학습될 수 있도록 돕는다.

**3. 디코더**
잠재 공간에서 샘플링된 벡터를 다시 원래 데이터 형태로 복원한다.


#### - Variational
VAE의 목표는 단순히 똑같은 이미지를 만드는 것이 아니라, 그럴듯한 새로운 데이터를 생성하는 것이다. 

**1.연속적인 잠재 공간**

데이터를 분포로 학습하기 때문에, 잠재 공간의 특정 지점들을 이동하며 데이터의 특징이 부드럽게 변하는 결과를 얻을 수 있다.


**2. 생성 능력**
학습이 끝난 후 디코더에 임의의 잠재 벡터를 넣어주면, 새로운 이미지를 생성한다. 

#### - 손실 함수
$$Loss = \mathcal{L}_{Recon} + \text{KL Divergence}$$

$$\mathcal_{L}_{Recon}$$ : 복원 손실. 출력값이 입력값과 얼마나 비슷한지를 측정

$$\text{KL Divergence}$$ : 학습된 잠재 공간의 분포가 사전에 가정한 가우시안 분포와 너무 멀어지지 않도록 규제.


## 3.2. Discrete Latent Variables
#### - Notation

$$K$$ : 이산 잠재 공간의 크기

$$D$$ : 각 임베딩 벡터 $$e_i$$ 의 차원(특징)

$$e_i$$ : 사전에 정의된 임베딩 벡터

$$e \in \mathbb{R}^{K \times D}$$ : 잠재 임베딩 공간

#### - 인코딩
입력값 $$x$$ 가 인코더 $$\mathcal{E}$$ 를 통과해 결과물 $$z_e(x)$$ 를 생성한다. 즉, 잠재 공간으로의 임베딩 벡터로 변환한다. 그리고 이산 잠재 변수인 $$z$$ 는 다음과 같은 식을 통해 사전에 정의된 임베딩 벡터 $$e$$ 로 근사된다.

$$
\begin{split}
  q(z=k \vert x) = 
  \begin{cases} 
    &1 \text{ for } k = argmin_j \Vert z_e(x) - e_i \Vert_{2} \\
    &0 \text{ otherwise }
  \end{cases}
\end{split}
$$

여기서 $$z_e(x)$$ 는 인코더를 통과한 후의 벡터이다. 이는 원-핫 인코딩을 이용한 사후 범주 확률 분포이다. 이후 디코더의 입력값은 다음과 같다.

$$
\begin{split}
  z_q(x) = e_k, \quad \text{where} \quad k=argmin_j \Vert z_e(x) - e_j \Vert_{2}
\end{split}
$$

모수가 존재하는 부분은 인코더, 디코더, 임베딩 공간까지 총 3개이다. 위와 같은 식들을 이용해 임베딩 벡터로 만드는 이유는 우리가 음성이나 이미지, 비디오와 같은 데이터에서 각각 1, 2, 3차원 데이터를 뽑고 이를 이산적으로 표현할 수 있기 때문이다. 필자들은 VAE를 ELBO로 $$\log p(x)$$ 의 경계를 설정할 수 있다는 부분에 초점을 맞춘다. 제안한 사후 범주 확률 분포는 결정적(deterministic)이고 $$z$$ 에 대한 간단한 균등 사전 분포르르 정의했기 때문에, KL 발산 상수가 $$\log K$$ 라는 사실을 알 수 있다.



## 3.3. Learning
$$z_q(x)$$ 의 경우 실제 기울기(gradient) 가 없기 때문에, Straight-Through Estimator (STE) 를 이용한다. 즉, 디코더 입력값인 $$z_q(x)$$ 부터 인코더의 결과값인 $$z_e(x)$$ 의 값을 복사해 사용한다.






# 4. Experiments







# 참고
https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows