---
layout: single        # 문서 형식
title: 'Generative Adversarial Nets 리뷰'         # 제목
categories: Generative Model    # 카테고리
toc: true             # 글 목차
author_profiel: false # 홈페이지 프로필이 다른 페이지에도 뜨는지 여부
sidebar:              # 페이지 왼쪽에 카테고리 지정
    nav: "docs"       # sidebar의 주소 지정
#search: false # 블로그 내 검색 비활성화
use_math: true
---
# Keywords
generative model, adversarial net, discriminator

# 1. Introduction

#### - 기존 생성모형의 어려움
생성모델은 MLE 등 확률적 계산을 추정하는 것과 생성한 context에 대해 구간별 선형 유닛의 이점을 활용하는 것이 어려워 상대적으로 영향력이 적었다.

#### - 제안 모형
본 논문에서 생성 모델은 데이터 분포로부터 생성하고 판별 모델은 모형이나 데이터의 분포에서 생성한 데이터가 진짜(real data)인지 아닌지(generated data)를 학습한다. 그리고 이 둘을 통해 역전파 및 최적화를 진행한다. 이 때, 기존에 사용하는 근사 추론이나 Markov chain 등은 필요하지 않다. 



# 2. Adversarial Nets
#### - Notation
$\mathbf{x}$ : 원본 데이터
$\mathbf{z}$ : 입력 noise 변수
$p_g$ : 생성모형의 분포
$p_\mathbf{z}(\mathbf{z})$ : $\mathbf{z}$의 사전 분포
$G$ : 생성 모델, 모수가 $\theta_g$인 다층 퍼셉트론으로 구성된 미분 가능한 함수 
$G(\mathbf{z} ;\theta_g)$ : 생성한 이미지의 분포
$D(\mathbf{x})$ : 판별 모델, $p_g$가 아닌 $\mathbf{x}$에서 추출되었을 확률분포
$D(\mathbf{x} ;\theta_g)$ : 원본 데이터로 구성된 공간 -> 출력 결과가 단일 스칼라인 두번째 다층 퍼셉트론 


## 2.1. Models
#### - Generative model $G$ 
원래 데이터 $x$의 분포를 근사할 수 있도록 학습한다. 만약 학습이 잘 되었다면 통계적으로 평균적인 특징을 가지는 데이터를 쉽게 생성 가능하다.

#### - Discriminator model $D$
데이터가 원래 데이터 $x$의 분포에서 나온 것인지, 아니면 $G$의 분포에서 나온 것인지(생성한 데이터인지) 판별하도록 학습한다. 출력 결과가 1(진짜) 또는 0(가짜)으로 판별한다.

#### - Objectives

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = '/images/GAN/figure1.jpg' width = 600>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure1 : Training sequences of Discriminator for time steps ]</figcaption>
</figure>

(a) -> (d) 로 시간의 흐름에 따라 진행하면서 생성 모델의 분포가 원본 데이터의 분포를 학습하는 것을 목표로 한다.


## 2.2. Objective Function
$$
\min_{G}  \max_{D} V(D,G) = \mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x}) }[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}(\mathbf{z}) }[\log 1-D(G(\mathbf{\mathbf{z}}))] 
$$

$\mathbb{E}_{\mathbf{x} \sim p_{data}({\mathbf{x}})} [logD(\mathbf{x})] : \mathbf{x}$ 에서 표본을 뽑아 $logD(x)$의 기댓값을 계산 
$\rightarrow$ 원본 데이터가 진짜(1)인지 가짜(0)인지 구분
$\rightarrow$ $ \max \atop D$ : 기댓값이 0이 나오도록 구성

$\mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}({\mathbf{z}})} [1-logD(G(\mathbf{z}))] : \mathbf{z}$ 에서 표본 $z$를 뽑아 $1-logD(G(z))$의 기댓값을 계산 
$\rightarrow$ 생성모델을 이용해 만든 이미지가 진짜(1)인지 가짜(0)인지 구분 
$\rightarrow$ $D(G(z))$ = 1 인 경우 기댓값 감소,  $D(G(z))$ = 0 인 경우 기댓값 증가
$\rightarrow$ $min \atop G$ : 기댓값이 0이 나오도록 구성



# 3. Theoritical Results
#### - algorithm 1 : 미니배치 최적화 진행 알고리즘

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/GAN/algorithm1.jpg" height = 400>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Algorithm1 : Minibatch SGD Training ]</figcaption>
</figure>


## 3.1. Global Optimality of $p_g = p_\mathbf{data}$
(생성한 이미지와 원본 이미지를 구별하지 않는 경우 또는 구별되지 않는 경우 고려)

#### - Proposition 1 : Optimal Discriminator
고정된 $G$에 대해, 최적의 판별자 D는 다음과 같다. 
$$
D^{*}_G(\mathbf{x}) = \frac {p_\mathbf{data}(\mathbf{x})}{p_\mathbf{data}(\mathbf{x}) + p_g(\mathbf{x})}
$$

$Proof$. 주어진 어떤 $G$에 대해,
$$ 
\begin{split}
V(G,D) &= \int_{\mathbf{x}}p_{data}(\mathbf{x})\log(D(\mathbf{x})) dx + \int_{\mathbf{z}}p_{\mathbf{z}}(\mathbf{z})\log(1-D(g(\mathbf{x}))) dz \\
&= \int_{\mathbf{x}}p_{data}(\mathbf{x})\log(D(\mathbf{x})) + p_{g}(\mathbf{x})\log(1-D(\mathbf{x}))dx
\end{split}
$$

$\because g(\mathbf{z}) = \mathbf{x} \quad if \quad p_g = p_\mathbf{data}\quad \rightarrow \quad $  이미지에 대한 labeling 구분이 없는 경우(또는 불가능한 경우)

$\forall (a,b) \in \mathbb{R}^2 \setminus \{0,0\}, \: F(y) = a\log(y) + b\log(1-y)$는 $a/(a+b)$ 에서 최댓값 [0,1]을 가진다. 따라서 판별 모형은 $Supp(p_{data}) \bigcup Supp(p_{g})$ 에서만 정의되므로 증명이 된다. $\square$

#### - Definition : The virtual training criterion $C(G)$
$$
\begin{split}
C(G) &= \max_{D} V(G,D) \\ 
&= \mathbb{E}_{\mathbf{x} \sim p_{data}} [\log D^*_G(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}} [1-\log D^*_G(G(\mathbf{z}))] \\
&= \mathbb{E}_{\mathbf{x} \sim p_{data}} [\log D^*_G(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{g}} [1-\log D^*_G(\mathbf{x})] \\
&= \mathbb{E}_{\mathbf{x} \sim p_{data}} \left[ \log \frac{p_{data}(\mathbf{x})}{p_{data}(\mathbf{x})+ p_{g}(\mathbf{x})} \right] + \mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}({\mathbf{z}})} \left[\log \frac{p_{g}(\mathbf{x})}{p_{data}(\mathbf{x})+ p_{g}(\mathbf{x})} \right]
\end{split}
$$

#### - Theorem 1 : Lower bound of the global minimum
$C(G)$의 전역(global) 최솟값의 필요충분 조건은 $p_g = p_\mathbf{data}$ 이고, 이 지점에서 $C(G)$의 값은 $-log4$이다.

$Proof$.
Proposition 1에 의해 $p_g = p_{data}$ 에 대해  $D^*_G(\mathbf{x}) = 1/2$ 이므로 $C(G) = log(1/2) + log(1/2) = -log4$ 이다. 위 정리(가설)에서 $C(G) = V(D^*_G, G)$ 로 표현하면, Jenson-Shannon divergence(JSD)에 의해

$$
\begin{split}
C(G) &= -\log 4 + KL(p_{data}|| \frac{p_{data}+ p_{g}}{2}) + KL(p_{g}|| \frac{p_{data}+ p_{g}}{2})\\  &= -\log 4 + 2 \cdot JSD(p_{data} || p_{g}) \\
\end{split}
$$

임을 알 수 있다. 두 분포간 $JSD$ 는 항상 0 이상이고 두 분포가 동일할 때 0 이므로, $ C(G) $ 의 전역 최솟값은 $- \log 4$ 임을 알 수 있다. $\square$


## 3.2. Convergence of Algorithm 1
#### - Proposition 2 : Covergence of $p_g$ to $p_x$ 
$p_g$는 $p_{data}$로 수렴하는 조건은 다음과 같다.
1. $G$와 $D$를 충분히 학습
2. Algorithm1의 각 단계에서 $D$가 주어진 $G$에 대해 최적값을 구성
3. $p_g$가 다음과 같은 기준으로 개선하도록 업데이트
$$
\mathbb{E}_{\mathbf{x} \sim p_{data}({\mathbf{x}})} [logD^*_G(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}({\mathbf{z}})} [1-logD^*_G(\mathbf{x})]
$$ 

$Proof.$
$V(G,D) = U(p_g,D)$ 에 대해 $U(p_g,D)$는 $p_g$에 대해 볼록함수이다. 그러면 볼록함수의 상한에 대한 하방미분은 최고점에서의 도함수를 포함한다. 즉, 이는 주어진 $G$ 에 대해 최적값을 갖는 $D$ 에서 $p_g$ 에 대한 gradient descent를 계산하는 것과 동일하다. Theorem 1에서 증명한 바와 같이 $\sup \atop D$ $U(p_g,D)$는 $p_g$에 대해 볼록하며 유일한 전역 최적값을 가진다. 따라서 충분히 적은 반복수로도 $p_g$ 가 $p_x$로 수렴하는 것을 알 수 있다. $\square$



# 4. Experiments
## 4.1. Train
#### - Adversarial nets
datasets : MNIST, Toronto Face Database(TFD), CIFAR-10

#### - Generator nets
rectifier linear activation(ReLU), sigmoid activation 사용

#### - Discriminator nets
maxout activation 사용, dropout 적용


## 4.2. Test
$G$로 생성한 샘플에 Gaussian Parzen window를 맞추어 이 분포 하에서 로그 가능도를 전달함으로써 테스트 데이터의 확률을 $p_g$ 하에서 추정했다. 이 때 Gaussian의 파라미터 $\sigma$는 cross validation을 이용해 최적의 값을 계산해 적용했다.

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/GAN/Table1.jpg" width="300" height = "100">
    <figcaption style="display: block; width: 100%; text-align: center;">[ Table1 : Parzen window-based log-likelihood estimates ]</figcaption>
</figure>


이 가능도 추정방법은 상당히 높은 분산을 가지고 고차원 데이터에 대한 성능이 좋지 않지만, 최선의 방법이다. 

<figure style="text-align: center; display: inline-block; width: 100%;">
    <img src = "/images/GAN/figure2.jpg" width = 500 height = 500>
    <figcaption style="display: block; width: 100%; text-align: center;">[ Figure2 : Visualizations of samples ]</figcaption>
</figure>

위 이미지들은 훈련 후 $G$ 의 표본들이다. 기존 방법들보다 우수하다고는 할 수는 없지만 적대적 프레임워크의 잠재력을 제시하며 우수한 생성 모델들과 경쟁이 가능하다고 본다.



# 5. Advantages and Disadvantages
#### - 장점
1. Markov chain이 필요하지않다.
2. 오직 역전파만을 이용해 기울기를 계산한다.
3. train 중 추론이 필요없으며 다양한 함수를 모델에 통합가능하다.
4. 주로 계산적인 측면에서 강점을 보인다.

#### - 단점
1. $p_g$에 대한 명시적 표현이 없다.
2. 훈련 중 $D$와 $G$가 잘 동기화되어야한다. $G$가 $D$를 잘 업데이트하지 않은 채로 훈련하게 되면, $p_{data}$를 모델링하기에는 다양성이 부족해지는 현상이 발생한다.



# 6. Conclusions and Future works 
1. 조건부 생성 모델 $p(\mathbf{x}|\mathbf{c})$는 $\mathbf{c}$ 를 $G$ 와 $D$ 모두에 입력으로 추가해 얻을 수 있다.
2. 학습된 근사 추론은 $\mathbf{x}$가 주어졌을 때 보조 네트워크를 훈련함으로써 $\mathbf{z}$ 를 예측할 수 있다.
3. $S$ 가 $\mathbf{x}$ 의 인덱스들의 하위 집합일때, 모든 조건부 $p(\mathbf{x}_S | \mathbf{x}_{\not{S}})$ 에 대해 매개변수를 공유하는 모든 조건부 모델 종류를 훈련해 대략적으로 모델링할 수 있다.
4. semi-supervised 학습 : 판별자 또는 추론 신경망에서 얻은 특징은 제한된 labeling 데이터가 있을 때 분류기의 성능을 향상할 수 있다.
5. 효율성 개선 : $G$와 $D$를 조정하거나 훈련 중 $\mathbf{z}$ 선택에 따라 훈련 속도를 크게 증가할 수 있다.



# 참고
[GAN: Generative Adversarial Networks (꼼꼼한 딥러닝 논문 리뷰와 코드 실습)]
https://www.youtube.com/watch?v=AVvlDmhHgC4&t=2020s

[Jensen-Shannon Divergence]
https://ddongwon.tistory.com/118